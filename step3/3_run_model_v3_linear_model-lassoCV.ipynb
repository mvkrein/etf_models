{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 500)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/mvkrein/etf_model/data'\n",
    "etf_data_file = os.path.join(data_path,'etf_new_var_20180910.csv')\n",
    "etf_data = pd.read_csv(etf_data_file,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_study_file = os.path.join(data_path,'ETF_list_min_6yr_history.csv')\n",
    "etf_list = pd.read_csv(etf_study_file,index_col=0)\n",
    "etf_data.sort_values(['Date','sym'],ascending=True,inplace=True)\n",
    "etf_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Date','sym','p', 'v', 'p_L05', 'v_L05', 'p_L10', 'v_L10', 'p_L21', 'v_L21', 'p_L42', 'v_L42', 'p_L63', \\\n",
    "                'v_L63', 'p_L84', 'v_L84', 'p_L126', 'v_L126', 'p_L189', 'v_L189', 'p_L252', 'v_L252',\\\n",
    "                'p_L-21', 'v_L-21', 'delta_p_L05', 'delta_p_L10', 'delta_p_L21', 'delta_p_L42', 'delta_p_L63', \\\n",
    "                'delta_p_L84', 'delta_p_L126', 'delta_p_L189', 'delta_p_L252', 'delta_p_L-21', 'delta_v_L05',\\\n",
    "                'delta_v_L10', 'delta_v_L21', 'delta_v_L42', 'delta_v_L63', 'delta_v_L84', 'delta_v_L126', \\\n",
    "                'delta_v_L189', 'delta_v_L252','rank_p_L-21', 'ivv_delta_p_L-21', 'target']\n",
    "\n",
    "rank_col = ['rank_p_L05','rank_p_L10','rank_p_L21','rank_p_L42',\\\n",
    "            'rank_p_L63','rank_p_L84','rank_p_L126','rank_p_L189','rank_p_L252']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = list(etf_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include_columns = [x for x in all_columns if x not in drop_columns]\n",
    "# include_columns = ['rank_p_L05','rank_p_L10','rank_p_L21','rank_p_L42',\\\n",
    "#             'rank_p_L63','rank_p_L84','rank_p_L126','rank_p_L189','rank_p_L252']\n",
    "# include_columns = ['rank_p_L05','rank_p_L21','rank_p_L42','rank_p_L252']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(1 % 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(include_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(etf_data['Date'].unique())\n",
    "# dates[1671]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = etf_list['Symbol'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each year has 252 trading dates.  Need two years to fully develop variables.\n",
    "dt1 = 504 #This is the first day that all variables are developed\n",
    "# dt1 = 504 + 21 + 273 #This is the first day that all variables are developed 2016-04-07\n",
    "# make all dates relative to dt1\n",
    "# for one year training - add 252\n",
    "# to evaluate for one month outside the training window - add 273\n",
    "# to predict for the first day outside of the evaluation window (have to lag 21) - add 294\n",
    "dt_end = (len(dates) - 273 - 21) - 1\n",
    "# dt_end = dt1 + 1\n",
    "etf_predict_file = os.path.join(data_path,'etf_linear_lasso_20180928.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model to predict for  2015-03-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=32)]: Done   2 out of   5 | elapsed:    3.3s remaining:    5.0s\n",
      ".....[Parallel(n_jobs=32)]: Done   5 out of   5 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
       "    max_iter=10000, n_alphas=100, n_jobs=32, normalize=False,\n",
       "    positive=False, precompute='auto', random_state=54321,\n",
       "    selection='cyclic', tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = dt1 + 21\n",
    "print(\"Building model to predict for \",dates[i+273])\n",
    "x_train = etf_data.loc[((etf_data['Date']>=dates[i]) & (etf_data['Date']<dates[i+252])),include_columns] #train with 12 mos\n",
    "y_train = etf_data.loc[((etf_data['Date']>=dates[i]) & (etf_data['Date']<dates[i+252])),['rank_p_L-21']] #train with 12 mos\n",
    "\n",
    "x_test = etf_data.loc[(etf_data['Date']==dates[i+273]),include_columns]#predict one day-must be 21 days removed from training\n",
    "y_test = etf_data.loc[(etf_data['Date']==dates[i+273]),['rank_p_L-21']] #predict if etf >= market\n",
    "returns = etf_data.loc[(etf_data['Date']==dates[i+273]),['delta_p_L-21']]\n",
    "mkt_return = etf_data.loc[(etf_data['Date']==dates[i+273]),['ivv_delta_p_L-21']]\n",
    "x_train_nmpy = x_train.as_matrix()\n",
    "y_train_nmpy = np.ravel(y_train.as_matrix())\n",
    "\n",
    "x_test_nmpy = x_test.as_matrix()\n",
    "y_test_nmpy = np.ravel(y_test.as_matrix())\n",
    "returns_nmpy = returns.as_matrix()\n",
    "mkt_return_nmpy = mkt_return.as_matrix()\n",
    "\n",
    "#     lm = linear_model.Ridge (alpha = 0.8)\n",
    "#     lm = linear_model.Lasso(alpha=0.0019)\n",
    "lm = LassoCV(cv=5,random_state=54321,verbose=1,n_jobs=32,max_iter=10000)\n",
    "#     lm = LinearRegression()\n",
    "\n",
    "lm.fit(x_train_nmpy, y_train_nmpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001922560911205997"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.120368</td>\n",
       "      <td>w42_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.056776</td>\n",
       "      <td>w252_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.055422</td>\n",
       "      <td>w5_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050248</td>\n",
       "      <td>w5_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028570</td>\n",
       "      <td>w42_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015645</td>\n",
       "      <td>rank_p_L63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.008797</td>\n",
       "      <td>w252_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>-0.014830</td>\n",
       "      <td>w5_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>-0.032587</td>\n",
       "      <td>w10_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>-0.034619</td>\n",
       "      <td>w84_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>-0.038034</td>\n",
       "      <td>w10_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>-0.040141</td>\n",
       "      <td>w84_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-0.051180</td>\n",
       "      <td>w10_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>-0.064792</td>\n",
       "      <td>w10_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coefficient               Feature\n",
       "0       0.120368    w42_rank_p_L42_avg\n",
       "1       0.056776  w252_rank_p_L252_avg\n",
       "2       0.055422     w5_rank_v_L63_avg\n",
       "3       0.050248     w5_rank_p_L63_avg\n",
       "4       0.028570    w42_rank_v_L42_avg\n",
       "5       0.015645            rank_p_L63\n",
       "6       0.008797  w252_rank_v_L252_avg\n",
       "353    -0.014830     w5_rank_v_L10_avg\n",
       "354    -0.032587    w10_rank_v_L84_avg\n",
       "355    -0.034619    w84_rank_v_L84_avg\n",
       "356    -0.038034    w10_rank_v_L21_avg\n",
       "357    -0.040141    w84_rank_p_L84_avg\n",
       "358    -0.051180    w10_rank_p_L84_avg\n",
       "359    -0.064792    w10_rank_p_L21_avg"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coef = pd.DataFrame({'Feature':include_columns,'Coefficient': lm.coef_})\n",
    "model_coef.sort_values(by='Coefficient',inplace=True,ascending=False) \n",
    "model_coef.reset_index(drop=True,inplace=True)\n",
    "model_coef.loc[abs(model_coef['Coefficient'])>0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model to predict for  2015-03-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=[100, 200, 300, 500, 1000, 5000], cv=5, fit_intercept=True,\n",
       "    gcv_mode=None, normalize=False, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = dt1 +21\n",
    "print(\"Building model to predict for \",dates[i+273])\n",
    "x_train = etf_data.loc[((etf_data['Date']>=dates[i]) & (etf_data['Date']<dates[i+252])),include_columns] #train with 12 mos\n",
    "y_train = etf_data.loc[((etf_data['Date']>=dates[i]) & (etf_data['Date']<dates[i+252])),['rank_p_L-21']] #train with 12 mos\n",
    "\n",
    "x_test = etf_data.loc[(etf_data['Date']==dates[i+273]),include_columns]#predict one day-must be 21 days removed from training\n",
    "y_test = etf_data.loc[(etf_data['Date']==dates[i+273]),['rank_p_L-21']] #predict if etf >= market\n",
    "returns = etf_data.loc[(etf_data['Date']==dates[i+273]),['delta_p_L-21']]\n",
    "mkt_return = etf_data.loc[(etf_data['Date']==dates[i+273]),['ivv_delta_p_L-21']]\n",
    "x_train_nmpy = x_train.as_matrix()\n",
    "y_train_nmpy = np.ravel(y_train.as_matrix())\n",
    "\n",
    "x_test_nmpy = x_test.as_matrix()\n",
    "y_test_nmpy = np.ravel(y_test.as_matrix())\n",
    "returns_nmpy = returns.as_matrix()\n",
    "mkt_return_nmpy = mkt_return.as_matrix()\n",
    "\n",
    "#     lm = linear_model.Ridge (alpha = 0.8)\n",
    "#     lm = linear_model.Lasso(alpha=0.0019)\n",
    "lm = RidgeCV(cv=5,alphas=[100,200,300,500,1000,5000])\n",
    "#     lm = LinearRegression()\n",
    "\n",
    "lm.fit(x_train_nmpy, y_train_nmpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045795</td>\n",
       "      <td>w252_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045795</td>\n",
       "      <td>w252_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037021</td>\n",
       "      <td>w42_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.037021</td>\n",
       "      <td>w42_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034843</td>\n",
       "      <td>w252_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.034843</td>\n",
       "      <td>w252_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.034223</td>\n",
       "      <td>w5_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.034223</td>\n",
       "      <td>w5_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.033577</td>\n",
       "      <td>w42_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.033577</td>\n",
       "      <td>w42_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.033061</td>\n",
       "      <td>w10_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.033061</td>\n",
       "      <td>w10_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.030929</td>\n",
       "      <td>rank_v_L63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.030929</td>\n",
       "      <td>rank_p_L63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.028335</td>\n",
       "      <td>w63_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.028335</td>\n",
       "      <td>w63_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.027695</td>\n",
       "      <td>w5_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.027695</td>\n",
       "      <td>w5_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.027020</td>\n",
       "      <td>rank_v_L126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.027020</td>\n",
       "      <td>rank_p_L126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.025527</td>\n",
       "      <td>w42_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.025527</td>\n",
       "      <td>w42_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.022997</td>\n",
       "      <td>w63_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.022997</td>\n",
       "      <td>w63_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.022922</td>\n",
       "      <td>w10_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.022922</td>\n",
       "      <td>w10_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.022847</td>\n",
       "      <td>w21_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.022847</td>\n",
       "      <td>w21_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.021442</td>\n",
       "      <td>w189_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.021442</td>\n",
       "      <td>w189_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.020480</td>\n",
       "      <td>w63_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.020480</td>\n",
       "      <td>w63_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.016520</td>\n",
       "      <td>w126_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.016520</td>\n",
       "      <td>w126_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.015602</td>\n",
       "      <td>w84_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.015602</td>\n",
       "      <td>w84_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.015103</td>\n",
       "      <td>w126_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.015103</td>\n",
       "      <td>w126_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>w21_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.014207</td>\n",
       "      <td>w21_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.013915</td>\n",
       "      <td>w126_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.013915</td>\n",
       "      <td>w126_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.013216</td>\n",
       "      <td>w63_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.013216</td>\n",
       "      <td>w63_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.012610</td>\n",
       "      <td>w21_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.012610</td>\n",
       "      <td>w21_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.010574</td>\n",
       "      <td>w252_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.010574</td>\n",
       "      <td>w252_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.009502</td>\n",
       "      <td>w42_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.009502</td>\n",
       "      <td>w42_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.008939</td>\n",
       "      <td>w42_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.008939</td>\n",
       "      <td>w42_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.008856</td>\n",
       "      <td>w21_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.008856</td>\n",
       "      <td>w21_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.006591</td>\n",
       "      <td>w252_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.006591</td>\n",
       "      <td>w252_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.006551</td>\n",
       "      <td>w252_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.006551</td>\n",
       "      <td>w252_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.006402</td>\n",
       "      <td>w189_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.006402</td>\n",
       "      <td>w189_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.006352</td>\n",
       "      <td>w63_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.006352</td>\n",
       "      <td>w63_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.006238</td>\n",
       "      <td>w42_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.006238</td>\n",
       "      <td>w42_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.005697</td>\n",
       "      <td>w189_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.005697</td>\n",
       "      <td>w189_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.004647</td>\n",
       "      <td>w5_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.004647</td>\n",
       "      <td>w5_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.003792</td>\n",
       "      <td>w63_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.003792</td>\n",
       "      <td>w63_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.003321</td>\n",
       "      <td>w189_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.003321</td>\n",
       "      <td>w189_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.002667</td>\n",
       "      <td>w42_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.002667</td>\n",
       "      <td>w42_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.002603</td>\n",
       "      <td>w126_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.002603</td>\n",
       "      <td>w126_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.002439</td>\n",
       "      <td>w189_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.002439</td>\n",
       "      <td>w189_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.001363</td>\n",
       "      <td>w252_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.001363</td>\n",
       "      <td>w252_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.000587</td>\n",
       "      <td>w10_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000587</td>\n",
       "      <td>w10_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000308</td>\n",
       "      <td>rank_v_L252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.000308</td>\n",
       "      <td>rank_p_L252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.000228</td>\n",
       "      <td>ivv_rank_p_L10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.000228</td>\n",
       "      <td>ivv_rank_v_L10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.000176</td>\n",
       "      <td>ivv_w21_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.000176</td>\n",
       "      <td>ivv_w21_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.000152</td>\n",
       "      <td>ivv_w10_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.000152</td>\n",
       "      <td>ivv_w10_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000143</td>\n",
       "      <td>ivv_w5_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.000143</td>\n",
       "      <td>ivv_w5_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.000141</td>\n",
       "      <td>ivv_w10_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.000141</td>\n",
       "      <td>ivv_w10_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.000126</td>\n",
       "      <td>ivv_w5_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.000126</td>\n",
       "      <td>ivv_w5_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.000118</td>\n",
       "      <td>ivv_w21_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000118</td>\n",
       "      <td>ivv_w21_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000098</td>\n",
       "      <td>ivv_w21_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.000098</td>\n",
       "      <td>ivv_w21_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>ivv_w10_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>ivv_w10_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.000066</td>\n",
       "      <td>ivv_w252_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.000066</td>\n",
       "      <td>ivv_w252_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.000065</td>\n",
       "      <td>ivv_w42_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.000065</td>\n",
       "      <td>ivv_w42_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>ivv_w42_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>ivv_w42_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.000059</td>\n",
       "      <td>ivv_w5_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.000059</td>\n",
       "      <td>ivv_w5_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>ivv_w252_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.000057</td>\n",
       "      <td>ivv_w252_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.000056</td>\n",
       "      <td>ivv_w42_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.000056</td>\n",
       "      <td>ivv_w42_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.000055</td>\n",
       "      <td>ivv_w10_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.000055</td>\n",
       "      <td>ivv_w10_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>ivv_w42_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>ivv_w42_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>ivv_w252_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>ivv_w252_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>ivv_w84_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>ivv_w84_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.000042</td>\n",
       "      <td>ivv_w84_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.000042</td>\n",
       "      <td>ivv_w84_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>ivv_w21_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>ivv_w21_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.000039</td>\n",
       "      <td>ivv_w84_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.000039</td>\n",
       "      <td>ivv_w84_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.000039</td>\n",
       "      <td>ivv_rank_v_L42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.000039</td>\n",
       "      <td>ivv_rank_p_L42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.000037</td>\n",
       "      <td>ivv_w42_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.000037</td>\n",
       "      <td>ivv_w42_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.000036</td>\n",
       "      <td>ivv_w5_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.000036</td>\n",
       "      <td>ivv_w5_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>ivv_w63_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>ivv_w63_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>ivv_w63_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>ivv_w63_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>ivv_w252_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>ivv_w252_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>ivv_w21_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>ivv_w21_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>ivv_w84_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>ivv_w84_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>ivv_w84_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>ivv_w84_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>ivv_w84_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>ivv_w84_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>ivv_w252_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>ivv_w252_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>ivv_rank_v_L126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>ivv_rank_p_L126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>ivv_w252_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>ivv_w252_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>ivv_w63_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>ivv_w63_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>ivv_w10_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>ivv_w10_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>ivv_w63_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>ivv_w63_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>ivv_w126_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>ivv_w126_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>ivv_rank_p_L63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>ivv_rank_v_L63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>ivv_w21_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.000007</td>\n",
       "      <td>ivv_w21_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>ivv_w63_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>ivv_w63_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>ivv_w10_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>ivv_w10_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>ivv_w21_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>ivv_w21_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>ivv_w5_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>ivv_w5_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>ivv_w5_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>ivv_w5_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-0.000002</td>\n",
       "      <td>ivv_rank_p_L252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.000002</td>\n",
       "      <td>ivv_rank_v_L252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>ivv_w84_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>ivv_w84_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>ivv_rank_p_L84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>ivv_rank_v_L84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>ivv_w126_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>-0.000005</td>\n",
       "      <td>ivv_w126_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>-0.000006</td>\n",
       "      <td>ivv_w189_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-0.000006</td>\n",
       "      <td>ivv_w189_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-0.000009</td>\n",
       "      <td>ivv_w189_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>-0.000009</td>\n",
       "      <td>ivv_w189_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>-0.000009</td>\n",
       "      <td>ivv_w63_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>-0.000009</td>\n",
       "      <td>ivv_w63_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>-0.000014</td>\n",
       "      <td>ivv_w42_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>-0.000014</td>\n",
       "      <td>ivv_w42_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-0.000014</td>\n",
       "      <td>ivv_w42_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>-0.000014</td>\n",
       "      <td>ivv_w42_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>-0.000015</td>\n",
       "      <td>ivv_w189_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-0.000015</td>\n",
       "      <td>ivv_w189_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-0.000015</td>\n",
       "      <td>ivv_w189_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.000015</td>\n",
       "      <td>ivv_w189_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.000016</td>\n",
       "      <td>ivv_w10_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-0.000016</td>\n",
       "      <td>ivv_w10_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.000018</td>\n",
       "      <td>ivv_w5_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-0.000018</td>\n",
       "      <td>ivv_w5_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-0.000018</td>\n",
       "      <td>ivv_w126_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>-0.000018</td>\n",
       "      <td>ivv_w126_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-0.000020</td>\n",
       "      <td>ivv_w126_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>-0.000020</td>\n",
       "      <td>ivv_w126_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>-0.000021</td>\n",
       "      <td>ivv_w126_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-0.000021</td>\n",
       "      <td>ivv_w126_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-0.000023</td>\n",
       "      <td>ivv_w63_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-0.000023</td>\n",
       "      <td>ivv_w63_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>-0.000027</td>\n",
       "      <td>ivv_w126_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>-0.000027</td>\n",
       "      <td>ivv_w126_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>-0.000029</td>\n",
       "      <td>ivv_w42_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>-0.000029</td>\n",
       "      <td>ivv_w42_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>-0.000030</td>\n",
       "      <td>ivv_w252_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>-0.000030</td>\n",
       "      <td>ivv_w252_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>-0.000036</td>\n",
       "      <td>ivv_w252_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>-0.000036</td>\n",
       "      <td>ivv_w252_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>-0.000039</td>\n",
       "      <td>ivv_w63_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>-0.000039</td>\n",
       "      <td>ivv_w63_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>ivv_w126_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>-0.000045</td>\n",
       "      <td>ivv_w126_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>-0.000047</td>\n",
       "      <td>ivv_w126_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>-0.000047</td>\n",
       "      <td>ivv_w126_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-0.000050</td>\n",
       "      <td>ivv_w189_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>-0.000050</td>\n",
       "      <td>ivv_w189_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>-0.000055</td>\n",
       "      <td>ivv_w252_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>-0.000055</td>\n",
       "      <td>ivv_w252_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>-0.000056</td>\n",
       "      <td>ivv_w189_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>-0.000056</td>\n",
       "      <td>ivv_w189_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>-0.000056</td>\n",
       "      <td>ivv_w189_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>-0.000056</td>\n",
       "      <td>ivv_w189_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-0.000062</td>\n",
       "      <td>ivv_w10_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>-0.000062</td>\n",
       "      <td>ivv_w10_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>-0.000063</td>\n",
       "      <td>ivv_w84_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>-0.000063</td>\n",
       "      <td>ivv_w84_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>-0.000063</td>\n",
       "      <td>ivv_w189_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>-0.000063</td>\n",
       "      <td>ivv_w189_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>-0.000071</td>\n",
       "      <td>ivv_w189_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>-0.000071</td>\n",
       "      <td>ivv_w189_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>-0.000075</td>\n",
       "      <td>ivv_rank_v_L21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-0.000075</td>\n",
       "      <td>ivv_rank_p_L21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>ivv_w84_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>-0.000077</td>\n",
       "      <td>ivv_w84_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>-0.000080</td>\n",
       "      <td>ivv_w126_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>-0.000080</td>\n",
       "      <td>ivv_w126_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>-0.000084</td>\n",
       "      <td>ivv_w21_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>-0.000084</td>\n",
       "      <td>ivv_w21_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-0.000094</td>\n",
       "      <td>ivv_w5_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.000094</td>\n",
       "      <td>ivv_w5_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>-0.000106</td>\n",
       "      <td>ivv_w5_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>-0.000106</td>\n",
       "      <td>ivv_w5_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>-0.000117</td>\n",
       "      <td>ivv_rank_v_L189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>-0.000117</td>\n",
       "      <td>ivv_rank_p_L189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>-0.000127</td>\n",
       "      <td>ivv_w10_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>-0.000127</td>\n",
       "      <td>ivv_w10_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>-0.000143</td>\n",
       "      <td>ivv_w63_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>-0.000143</td>\n",
       "      <td>ivv_w63_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>-0.000149</td>\n",
       "      <td>rank_p_L10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>-0.000149</td>\n",
       "      <td>rank_v_L10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>-0.000175</td>\n",
       "      <td>ivv_w21_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>-0.000175</td>\n",
       "      <td>ivv_w21_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>-0.000181</td>\n",
       "      <td>ivv_w42_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>-0.000181</td>\n",
       "      <td>ivv_w42_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-0.000225</td>\n",
       "      <td>ivv_rank_v_L05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>-0.000225</td>\n",
       "      <td>ivv_rank_p_L05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>-0.000480</td>\n",
       "      <td>w189_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>-0.000480</td>\n",
       "      <td>w189_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-0.000516</td>\n",
       "      <td>w21_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>-0.000516</td>\n",
       "      <td>w21_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-0.000653</td>\n",
       "      <td>rank_p_L21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>-0.000653</td>\n",
       "      <td>rank_v_L21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>-0.000757</td>\n",
       "      <td>w189_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>-0.000757</td>\n",
       "      <td>w189_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-0.001727</td>\n",
       "      <td>rank_p_L05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.001727</td>\n",
       "      <td>rank_v_L05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>-0.001755</td>\n",
       "      <td>w252_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>-0.001755</td>\n",
       "      <td>w252_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>-0.003693</td>\n",
       "      <td>w126_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>-0.003693</td>\n",
       "      <td>w126_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>-0.003987</td>\n",
       "      <td>w5_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>-0.003987</td>\n",
       "      <td>w5_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>-0.004157</td>\n",
       "      <td>w84_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>-0.004157</td>\n",
       "      <td>w84_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>-0.005302</td>\n",
       "      <td>w10_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>-0.005302</td>\n",
       "      <td>w10_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-0.005406</td>\n",
       "      <td>w5_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>-0.005406</td>\n",
       "      <td>w5_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-0.005432</td>\n",
       "      <td>rank_v_L189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>-0.005432</td>\n",
       "      <td>rank_p_L189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>-0.006321</td>\n",
       "      <td>w126_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>-0.006321</td>\n",
       "      <td>w126_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>-0.006598</td>\n",
       "      <td>w10_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>-0.006598</td>\n",
       "      <td>w10_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>-0.006894</td>\n",
       "      <td>w5_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-0.006894</td>\n",
       "      <td>w5_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.007023</td>\n",
       "      <td>w42_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>-0.007023</td>\n",
       "      <td>w42_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.007507</td>\n",
       "      <td>w63_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.007507</td>\n",
       "      <td>w63_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.007613</td>\n",
       "      <td>w84_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-0.007613</td>\n",
       "      <td>w84_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-0.008416</td>\n",
       "      <td>w5_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-0.008416</td>\n",
       "      <td>w5_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-0.008545</td>\n",
       "      <td>w5_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-0.008545</td>\n",
       "      <td>w5_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>-0.009365</td>\n",
       "      <td>w21_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-0.009365</td>\n",
       "      <td>w21_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-0.009850</td>\n",
       "      <td>rank_v_L42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-0.009850</td>\n",
       "      <td>rank_p_L42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>-0.010275</td>\n",
       "      <td>w126_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-0.010275</td>\n",
       "      <td>w126_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>-0.011128</td>\n",
       "      <td>w84_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>-0.011128</td>\n",
       "      <td>w84_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>-0.011248</td>\n",
       "      <td>w126_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>-0.011248</td>\n",
       "      <td>w126_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>-0.011493</td>\n",
       "      <td>w189_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>-0.011493</td>\n",
       "      <td>w189_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-0.012226</td>\n",
       "      <td>w84_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>-0.012226</td>\n",
       "      <td>w84_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>-0.012491</td>\n",
       "      <td>w10_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>-0.012491</td>\n",
       "      <td>w10_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>-0.012749</td>\n",
       "      <td>w10_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>-0.012749</td>\n",
       "      <td>w10_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>-0.014341</td>\n",
       "      <td>w21_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>-0.014341</td>\n",
       "      <td>w21_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>-0.014987</td>\n",
       "      <td>w189_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>-0.014987</td>\n",
       "      <td>w189_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>-0.015572</td>\n",
       "      <td>w42_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>-0.015572</td>\n",
       "      <td>w42_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>-0.019731</td>\n",
       "      <td>w126_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>-0.019731</td>\n",
       "      <td>w126_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>-0.020073</td>\n",
       "      <td>w63_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>-0.020073</td>\n",
       "      <td>w63_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>-0.020318</td>\n",
       "      <td>rank_p_L84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>-0.020318</td>\n",
       "      <td>rank_v_L84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-0.021685</td>\n",
       "      <td>w84_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>-0.021685</td>\n",
       "      <td>w84_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>-0.022145</td>\n",
       "      <td>w252_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>-0.022145</td>\n",
       "      <td>w252_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>-0.024473</td>\n",
       "      <td>w84_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>-0.024473</td>\n",
       "      <td>w84_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>-0.026892</td>\n",
       "      <td>w5_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>-0.026892</td>\n",
       "      <td>w5_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-0.026920</td>\n",
       "      <td>w252_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>-0.026920</td>\n",
       "      <td>w252_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>-0.027659</td>\n",
       "      <td>w10_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>-0.027659</td>\n",
       "      <td>w10_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>-0.029732</td>\n",
       "      <td>w63_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>-0.029732</td>\n",
       "      <td>w63_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>-0.030085</td>\n",
       "      <td>w84_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>-0.030085</td>\n",
       "      <td>w84_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>-0.030604</td>\n",
       "      <td>w21_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>-0.030604</td>\n",
       "      <td>w21_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>-0.031776</td>\n",
       "      <td>w21_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>-0.031776</td>\n",
       "      <td>w21_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>-0.033873</td>\n",
       "      <td>w84_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>-0.033873</td>\n",
       "      <td>w84_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-0.034053</td>\n",
       "      <td>w10_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>-0.034053</td>\n",
       "      <td>w10_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coefficient                   Feature\n",
       "0       0.045795      w252_rank_v_L252_avg\n",
       "1       0.045795      w252_rank_p_L252_avg\n",
       "2       0.037021        w42_rank_p_L42_avg\n",
       "3       0.037021        w42_rank_v_L42_avg\n",
       "4       0.034843      w252_rank_v_L189_avg\n",
       "5       0.034843      w252_rank_p_L189_avg\n",
       "6       0.034223         w5_rank_v_L63_avg\n",
       "7       0.034223         w5_rank_p_L63_avg\n",
       "8       0.033577       w42_rank_p_L189_avg\n",
       "9       0.033577       w42_rank_v_L189_avg\n",
       "10      0.033061        w10_rank_v_L63_avg\n",
       "11      0.033061        w10_rank_p_L63_avg\n",
       "12      0.030929                rank_v_L63\n",
       "13      0.030929                rank_p_L63\n",
       "14      0.028335       w63_rank_v_L189_avg\n",
       "15      0.028335       w63_rank_p_L189_avg\n",
       "16      0.027695        w5_rank_v_L126_avg\n",
       "17      0.027695        w5_rank_p_L126_avg\n",
       "18      0.027020               rank_v_L126\n",
       "19      0.027020               rank_p_L126\n",
       "20      0.025527        w42_rank_p_L21_avg\n",
       "21      0.025527        w42_rank_v_L21_avg\n",
       "22      0.022997        w63_rank_v_L10_avg\n",
       "23      0.022997        w63_rank_p_L10_avg\n",
       "24      0.022922       w10_rank_v_L126_avg\n",
       "25      0.022922       w10_rank_p_L126_avg\n",
       "26      0.022847       w21_rank_p_L189_avg\n",
       "27      0.022847       w21_rank_v_L189_avg\n",
       "28      0.021442      w189_rank_v_L252_avg\n",
       "29      0.021442      w189_rank_p_L252_avg\n",
       "30      0.020480        w63_rank_p_L05_avg\n",
       "31      0.020480        w63_rank_v_L05_avg\n",
       "32      0.016520       w126_rank_v_L10_avg\n",
       "33      0.016520       w126_rank_p_L10_avg\n",
       "34      0.015602       w84_rank_v_L189_avg\n",
       "35      0.015602       w84_rank_p_L189_avg\n",
       "36      0.015103       w126_rank_v_L05_avg\n",
       "37      0.015103       w126_rank_p_L05_avg\n",
       "38      0.014207        w21_rank_p_L42_avg\n",
       "39      0.014207        w21_rank_v_L42_avg\n",
       "40      0.013915       w126_rank_p_L21_avg\n",
       "41      0.013915       w126_rank_v_L21_avg\n",
       "42      0.013216        w63_rank_v_L21_avg\n",
       "43      0.013216        w63_rank_p_L21_avg\n",
       "44      0.012610        w21_rank_v_L63_avg\n",
       "45      0.012610        w21_rank_p_L63_avg\n",
       "46      0.010574       w252_rank_v_L21_avg\n",
       "47      0.010574       w252_rank_p_L21_avg\n",
       "48      0.009502       w42_rank_p_L252_avg\n",
       "49      0.009502       w42_rank_v_L252_avg\n",
       "50      0.008939        w42_rank_p_L84_avg\n",
       "51      0.008939        w42_rank_v_L84_avg\n",
       "52      0.008856       w21_rank_p_L252_avg\n",
       "53      0.008856       w21_rank_v_L252_avg\n",
       "54      0.006591       w252_rank_v_L05_avg\n",
       "55      0.006591       w252_rank_p_L05_avg\n",
       "56      0.006551       w252_rank_v_L10_avg\n",
       "57      0.006551       w252_rank_p_L10_avg\n",
       "58      0.006402       w189_rank_p_L05_avg\n",
       "59      0.006402       w189_rank_v_L05_avg\n",
       "60      0.006352        w63_rank_p_L42_avg\n",
       "61      0.006352        w63_rank_v_L42_avg\n",
       "62      0.006238        w42_rank_p_L10_avg\n",
       "63      0.006238        w42_rank_v_L10_avg\n",
       "64      0.005697       w189_rank_p_L21_avg\n",
       "65      0.005697       w189_rank_v_L21_avg\n",
       "66      0.004647         w5_rank_p_L05_avg\n",
       "67      0.004647         w5_rank_v_L05_avg\n",
       "68      0.003792        w63_rank_v_L63_avg\n",
       "69      0.003792        w63_rank_p_L63_avg\n",
       "70      0.003321       w189_rank_v_L10_avg\n",
       "71      0.003321       w189_rank_p_L10_avg\n",
       "72      0.002667        w42_rank_v_L63_avg\n",
       "73      0.002667        w42_rank_p_L63_avg\n",
       "74      0.002603       w126_rank_v_L42_avg\n",
       "75      0.002603       w126_rank_p_L42_avg\n",
       "76      0.002439      w189_rank_p_L189_avg\n",
       "77      0.002439      w189_rank_v_L189_avg\n",
       "78      0.001363       w252_rank_p_L42_avg\n",
       "79      0.001363       w252_rank_v_L42_avg\n",
       "80      0.000587       w10_rank_p_L189_avg\n",
       "81      0.000587       w10_rank_v_L189_avg\n",
       "82      0.000308               rank_v_L252\n",
       "83      0.000308               rank_p_L252\n",
       "84      0.000228            ivv_rank_p_L10\n",
       "85      0.000228            ivv_rank_v_L10\n",
       "86      0.000176    ivv_w21_rank_p_L21_avg\n",
       "87      0.000176    ivv_w21_rank_v_L21_avg\n",
       "88      0.000152    ivv_w10_rank_v_L21_avg\n",
       "89      0.000152    ivv_w10_rank_p_L21_avg\n",
       "90      0.000143     ivv_w5_rank_v_L42_avg\n",
       "91      0.000143     ivv_w5_rank_p_L42_avg\n",
       "92      0.000141    ivv_w10_rank_v_L10_avg\n",
       "93      0.000141    ivv_w10_rank_p_L10_avg\n",
       "94      0.000126     ivv_w5_rank_p_L10_avg\n",
       "95      0.000126     ivv_w5_rank_v_L10_avg\n",
       "96      0.000118    ivv_w21_rank_p_L10_avg\n",
       "97      0.000118    ivv_w21_rank_v_L10_avg\n",
       "98      0.000098    ivv_w21_rank_p_L42_avg\n",
       "99      0.000098    ivv_w21_rank_v_L42_avg\n",
       "100     0.000090    ivv_w10_rank_p_L42_avg\n",
       "101     0.000090    ivv_w10_rank_v_L42_avg\n",
       "102     0.000066   ivv_w252_rank_p_L42_avg\n",
       "103     0.000066   ivv_w252_rank_v_L42_avg\n",
       "104     0.000065    ivv_w42_rank_v_L21_avg\n",
       "105     0.000065    ivv_w42_rank_p_L21_avg\n",
       "106     0.000062    ivv_w42_rank_p_L10_avg\n",
       "107     0.000062    ivv_w42_rank_v_L10_avg\n",
       "108     0.000059     ivv_w5_rank_p_L63_avg\n",
       "109     0.000059     ivv_w5_rank_v_L63_avg\n",
       "110     0.000057   ivv_w252_rank_p_L21_avg\n",
       "111     0.000057   ivv_w252_rank_v_L21_avg\n",
       "112     0.000056   ivv_w42_rank_p_L252_avg\n",
       "113     0.000056   ivv_w42_rank_v_L252_avg\n",
       "114     0.000055    ivv_w10_rank_p_L84_avg\n",
       "115     0.000055    ivv_w10_rank_v_L84_avg\n",
       "116     0.000052    ivv_w42_rank_v_L05_avg\n",
       "117     0.000052    ivv_w42_rank_p_L05_avg\n",
       "118     0.000051   ivv_w252_rank_v_L63_avg\n",
       "119     0.000051   ivv_w252_rank_p_L63_avg\n",
       "120     0.000043    ivv_w84_rank_v_L10_avg\n",
       "121     0.000043    ivv_w84_rank_p_L10_avg\n",
       "122     0.000042    ivv_w84_rank_p_L21_avg\n",
       "123     0.000042    ivv_w84_rank_v_L21_avg\n",
       "124     0.000040   ivv_w21_rank_p_L252_avg\n",
       "125     0.000040   ivv_w21_rank_v_L252_avg\n",
       "126     0.000039    ivv_w84_rank_v_L63_avg\n",
       "127     0.000039    ivv_w84_rank_p_L63_avg\n",
       "128     0.000039            ivv_rank_v_L42\n",
       "129     0.000039            ivv_rank_p_L42\n",
       "130     0.000037    ivv_w42_rank_p_L84_avg\n",
       "131     0.000037    ivv_w42_rank_v_L84_avg\n",
       "132     0.000036     ivv_w5_rank_p_L84_avg\n",
       "133     0.000036     ivv_w5_rank_v_L84_avg\n",
       "134     0.000032    ivv_w63_rank_v_L05_avg\n",
       "135     0.000032    ivv_w63_rank_p_L05_avg\n",
       "136     0.000030   ivv_w63_rank_v_L252_avg\n",
       "137     0.000030   ivv_w63_rank_p_L252_avg\n",
       "138     0.000030   ivv_w252_rank_p_L10_avg\n",
       "139     0.000030   ivv_w252_rank_v_L10_avg\n",
       "140     0.000027    ivv_w21_rank_v_L84_avg\n",
       "141     0.000027    ivv_w21_rank_p_L84_avg\n",
       "142     0.000027    ivv_w84_rank_v_L84_avg\n",
       "143     0.000027    ivv_w84_rank_p_L84_avg\n",
       "144     0.000026    ivv_w84_rank_p_L42_avg\n",
       "145     0.000026    ivv_w84_rank_v_L42_avg\n",
       "146     0.000024    ivv_w84_rank_p_L05_avg\n",
       "147     0.000024    ivv_w84_rank_v_L05_avg\n",
       "148     0.000022   ivv_w252_rank_v_L84_avg\n",
       "149     0.000022   ivv_w252_rank_p_L84_avg\n",
       "150     0.000018           ivv_rank_v_L126\n",
       "151     0.000018           ivv_rank_p_L126\n",
       "152     0.000018   ivv_w252_rank_v_L05_avg\n",
       "153     0.000018   ivv_w252_rank_p_L05_avg\n",
       "154     0.000015    ivv_w63_rank_v_L63_avg\n",
       "155     0.000015    ivv_w63_rank_p_L63_avg\n",
       "156     0.000015   ivv_w10_rank_v_L252_avg\n",
       "157     0.000015   ivv_w10_rank_p_L252_avg\n",
       "158     0.000013    ivv_w63_rank_p_L84_avg\n",
       "159     0.000013    ivv_w63_rank_v_L84_avg\n",
       "160     0.000010   ivv_w126_rank_v_L21_avg\n",
       "161     0.000010   ivv_w126_rank_p_L21_avg\n",
       "162     0.000007            ivv_rank_p_L63\n",
       "163     0.000007            ivv_rank_v_L63\n",
       "164     0.000007    ivv_w21_rank_p_L05_avg\n",
       "165     0.000007    ivv_w21_rank_v_L05_avg\n",
       "166     0.000006    ivv_w63_rank_v_L10_avg\n",
       "167     0.000006    ivv_w63_rank_p_L10_avg\n",
       "168     0.000005   ivv_w10_rank_p_L126_avg\n",
       "169     0.000005   ivv_w10_rank_v_L126_avg\n",
       "170     0.000005   ivv_w21_rank_v_L126_avg\n",
       "171     0.000005   ivv_w21_rank_p_L126_avg\n",
       "172     0.000002    ivv_w5_rank_p_L252_avg\n",
       "173     0.000002    ivv_w5_rank_v_L252_avg\n",
       "174     0.000001    ivv_w5_rank_p_L126_avg\n",
       "175     0.000001    ivv_w5_rank_v_L126_avg\n",
       "176    -0.000002           ivv_rank_p_L252\n",
       "177    -0.000002           ivv_rank_v_L252\n",
       "178    -0.000003   ivv_w84_rank_p_L252_avg\n",
       "179    -0.000003   ivv_w84_rank_v_L252_avg\n",
       "180    -0.000005            ivv_rank_p_L84\n",
       "181    -0.000005            ivv_rank_v_L84\n",
       "182    -0.000005   ivv_w126_rank_p_L10_avg\n",
       "183    -0.000005   ivv_w126_rank_v_L10_avg\n",
       "184    -0.000006   ivv_w189_rank_v_L05_avg\n",
       "185    -0.000006   ivv_w189_rank_p_L05_avg\n",
       "186    -0.000009   ivv_w189_rank_v_L10_avg\n",
       "187    -0.000009   ivv_w189_rank_p_L10_avg\n",
       "188    -0.000009    ivv_w63_rank_p_L21_avg\n",
       "189    -0.000009    ivv_w63_rank_v_L21_avg\n",
       "190    -0.000014   ivv_w42_rank_p_L126_avg\n",
       "191    -0.000014   ivv_w42_rank_v_L126_avg\n",
       "192    -0.000014    ivv_w42_rank_p_L42_avg\n",
       "193    -0.000014    ivv_w42_rank_v_L42_avg\n",
       "194    -0.000015  ivv_w189_rank_v_L126_avg\n",
       "195    -0.000015  ivv_w189_rank_p_L126_avg\n",
       "196    -0.000015   ivv_w189_rank_p_L84_avg\n",
       "197    -0.000015   ivv_w189_rank_v_L84_avg\n",
       "198    -0.000016    ivv_w10_rank_v_L05_avg\n",
       "199    -0.000016    ivv_w10_rank_p_L05_avg\n",
       "200    -0.000018     ivv_w5_rank_v_L21_avg\n",
       "201    -0.000018     ivv_w5_rank_p_L21_avg\n",
       "202    -0.000018   ivv_w126_rank_v_L05_avg\n",
       "203    -0.000018   ivv_w126_rank_p_L05_avg\n",
       "204    -0.000020   ivv_w126_rank_v_L63_avg\n",
       "205    -0.000020   ivv_w126_rank_p_L63_avg\n",
       "206    -0.000021   ivv_w126_rank_p_L42_avg\n",
       "207    -0.000021   ivv_w126_rank_v_L42_avg\n",
       "208    -0.000023    ivv_w63_rank_p_L42_avg\n",
       "209    -0.000023    ivv_w63_rank_v_L42_avg\n",
       "210    -0.000027  ivv_w126_rank_v_L189_avg\n",
       "211    -0.000027  ivv_w126_rank_p_L189_avg\n",
       "212    -0.000029    ivv_w42_rank_p_L63_avg\n",
       "213    -0.000029    ivv_w42_rank_v_L63_avg\n",
       "214    -0.000030  ivv_w252_rank_v_L252_avg\n",
       "215    -0.000030  ivv_w252_rank_p_L252_avg\n",
       "216    -0.000036  ivv_w252_rank_p_L126_avg\n",
       "217    -0.000036  ivv_w252_rank_v_L126_avg\n",
       "218    -0.000039   ivv_w63_rank_p_L126_avg\n",
       "219    -0.000039   ivv_w63_rank_v_L126_avg\n",
       "220    -0.000045   ivv_w126_rank_v_L84_avg\n",
       "221    -0.000045   ivv_w126_rank_p_L84_avg\n",
       "222    -0.000047  ivv_w126_rank_p_L252_avg\n",
       "223    -0.000047  ivv_w126_rank_v_L252_avg\n",
       "224    -0.000050   ivv_w189_rank_v_L21_avg\n",
       "225    -0.000050   ivv_w189_rank_p_L21_avg\n",
       "226    -0.000055  ivv_w252_rank_p_L189_avg\n",
       "227    -0.000055  ivv_w252_rank_v_L189_avg\n",
       "228    -0.000056  ivv_w189_rank_v_L252_avg\n",
       "229    -0.000056  ivv_w189_rank_p_L252_avg\n",
       "230    -0.000056   ivv_w189_rank_v_L63_avg\n",
       "231    -0.000056   ivv_w189_rank_p_L63_avg\n",
       "232    -0.000062    ivv_w10_rank_v_L63_avg\n",
       "233    -0.000062    ivv_w10_rank_p_L63_avg\n",
       "234    -0.000063   ivv_w84_rank_p_L189_avg\n",
       "235    -0.000063   ivv_w84_rank_v_L189_avg\n",
       "236    -0.000063  ivv_w189_rank_v_L189_avg\n",
       "237    -0.000063  ivv_w189_rank_p_L189_avg\n",
       "238    -0.000071   ivv_w189_rank_p_L42_avg\n",
       "239    -0.000071   ivv_w189_rank_v_L42_avg\n",
       "240    -0.000075            ivv_rank_v_L21\n",
       "241    -0.000075            ivv_rank_p_L21\n",
       "242    -0.000077   ivv_w84_rank_p_L126_avg\n",
       "243    -0.000077   ivv_w84_rank_v_L126_avg\n",
       "244    -0.000080  ivv_w126_rank_p_L126_avg\n",
       "245    -0.000080  ivv_w126_rank_v_L126_avg\n",
       "246    -0.000084    ivv_w21_rank_p_L63_avg\n",
       "247    -0.000084    ivv_w21_rank_v_L63_avg\n",
       "248    -0.000094     ivv_w5_rank_v_L05_avg\n",
       "249    -0.000094     ivv_w5_rank_p_L05_avg\n",
       "250    -0.000106    ivv_w5_rank_v_L189_avg\n",
       "251    -0.000106    ivv_w5_rank_p_L189_avg\n",
       "252    -0.000117           ivv_rank_v_L189\n",
       "253    -0.000117           ivv_rank_p_L189\n",
       "254    -0.000127   ivv_w10_rank_p_L189_avg\n",
       "255    -0.000127   ivv_w10_rank_v_L189_avg\n",
       "256    -0.000143   ivv_w63_rank_v_L189_avg\n",
       "257    -0.000143   ivv_w63_rank_p_L189_avg\n",
       "258    -0.000149                rank_p_L10\n",
       "259    -0.000149                rank_v_L10\n",
       "260    -0.000175   ivv_w21_rank_p_L189_avg\n",
       "261    -0.000175   ivv_w21_rank_v_L189_avg\n",
       "262    -0.000181   ivv_w42_rank_v_L189_avg\n",
       "263    -0.000181   ivv_w42_rank_p_L189_avg\n",
       "264    -0.000225            ivv_rank_v_L05\n",
       "265    -0.000225            ivv_rank_p_L05\n",
       "266    -0.000480       w189_rank_v_L42_avg\n",
       "267    -0.000480       w189_rank_p_L42_avg\n",
       "268    -0.000516       w21_rank_p_L126_avg\n",
       "269    -0.000516       w21_rank_v_L126_avg\n",
       "270    -0.000653                rank_p_L21\n",
       "271    -0.000653                rank_v_L21\n",
       "272    -0.000757       w189_rank_p_L63_avg\n",
       "273    -0.000757       w189_rank_v_L63_avg\n",
       "274    -0.001727                rank_p_L05\n",
       "275    -0.001727                rank_v_L05\n",
       "276    -0.001755      w252_rank_p_L126_avg\n",
       "277    -0.001755      w252_rank_v_L126_avg\n",
       "278    -0.003693       w126_rank_p_L63_avg\n",
       "279    -0.003693       w126_rank_v_L63_avg\n",
       "280    -0.003987        w5_rank_v_L252_avg\n",
       "281    -0.003987        w5_rank_p_L252_avg\n",
       "282    -0.004157        w84_rank_p_L05_avg\n",
       "283    -0.004157        w84_rank_v_L05_avg\n",
       "284    -0.005302        w10_rank_v_L42_avg\n",
       "285    -0.005302        w10_rank_p_L42_avg\n",
       "286    -0.005406        w5_rank_p_L189_avg\n",
       "287    -0.005406        w5_rank_v_L189_avg\n",
       "288    -0.005432               rank_v_L189\n",
       "289    -0.005432               rank_p_L189\n",
       "290    -0.006321      w126_rank_p_L189_avg\n",
       "291    -0.006321      w126_rank_v_L189_avg\n",
       "292    -0.006598       w10_rank_p_L252_avg\n",
       "293    -0.006598       w10_rank_v_L252_avg\n",
       "294    -0.006894         w5_rank_v_L10_avg\n",
       "295    -0.006894         w5_rank_p_L10_avg\n",
       "296    -0.007023        w42_rank_v_L05_avg\n",
       "297    -0.007023        w42_rank_p_L05_avg\n",
       "298    -0.007507        w63_rank_p_L84_avg\n",
       "299    -0.007507        w63_rank_v_L84_avg\n",
       "300    -0.007613        w84_rank_v_L42_avg\n",
       "301    -0.007613        w84_rank_p_L42_avg\n",
       "302    -0.008416         w5_rank_p_L21_avg\n",
       "303    -0.008416         w5_rank_v_L21_avg\n",
       "304    -0.008545         w5_rank_p_L42_avg\n",
       "305    -0.008545         w5_rank_v_L42_avg\n",
       "306    -0.009365        w21_rank_p_L05_avg\n",
       "307    -0.009365        w21_rank_v_L05_avg\n",
       "308    -0.009850                rank_v_L42\n",
       "309    -0.009850                rank_p_L42\n",
       "310    -0.010275      w126_rank_v_L126_avg\n",
       "311    -0.010275      w126_rank_p_L126_avg\n",
       "312    -0.011128        w84_rank_v_L63_avg\n",
       "313    -0.011128        w84_rank_p_L63_avg\n",
       "314    -0.011248      w126_rank_p_L252_avg\n",
       "315    -0.011248      w126_rank_v_L252_avg\n",
       "316    -0.011493       w189_rank_p_L84_avg\n",
       "317    -0.011493       w189_rank_v_L84_avg\n",
       "318    -0.012226        w84_rank_p_L10_avg\n",
       "319    -0.012226        w84_rank_v_L10_avg\n",
       "320    -0.012491        w10_rank_p_L05_avg\n",
       "321    -0.012491        w10_rank_v_L05_avg\n",
       "322    -0.012749        w10_rank_p_L10_avg\n",
       "323    -0.012749        w10_rank_v_L10_avg\n",
       "324    -0.014341        w21_rank_p_L10_avg\n",
       "325    -0.014341        w21_rank_v_L10_avg\n",
       "326    -0.014987      w189_rank_v_L126_avg\n",
       "327    -0.014987      w189_rank_p_L126_avg\n",
       "328    -0.015572       w42_rank_v_L126_avg\n",
       "329    -0.015572       w42_rank_p_L126_avg\n",
       "330    -0.019731       w126_rank_v_L84_avg\n",
       "331    -0.019731       w126_rank_p_L84_avg\n",
       "332    -0.020073       w63_rank_p_L126_avg\n",
       "333    -0.020073       w63_rank_v_L126_avg\n",
       "334    -0.020318                rank_p_L84\n",
       "335    -0.020318                rank_v_L84\n",
       "336    -0.021685        w84_rank_v_L21_avg\n",
       "337    -0.021685        w84_rank_p_L21_avg\n",
       "338    -0.022145       w252_rank_v_L63_avg\n",
       "339    -0.022145       w252_rank_p_L63_avg\n",
       "340    -0.024473       w84_rank_v_L126_avg\n",
       "341    -0.024473       w84_rank_p_L126_avg\n",
       "342    -0.026892         w5_rank_v_L84_avg\n",
       "343    -0.026892         w5_rank_p_L84_avg\n",
       "344    -0.026920       w252_rank_p_L84_avg\n",
       "345    -0.026920       w252_rank_v_L84_avg\n",
       "346    -0.027659        w10_rank_p_L21_avg\n",
       "347    -0.027659        w10_rank_v_L21_avg\n",
       "348    -0.029732       w63_rank_p_L252_avg\n",
       "349    -0.029732       w63_rank_v_L252_avg\n",
       "350    -0.030085       w84_rank_v_L252_avg\n",
       "351    -0.030085       w84_rank_p_L252_avg\n",
       "352    -0.030604        w21_rank_p_L21_avg\n",
       "353    -0.030604        w21_rank_v_L21_avg\n",
       "354    -0.031776        w21_rank_p_L84_avg\n",
       "355    -0.031776        w21_rank_v_L84_avg\n",
       "356    -0.033873        w84_rank_v_L84_avg\n",
       "357    -0.033873        w84_rank_p_L84_avg\n",
       "358    -0.034053        w10_rank_p_L84_avg\n",
       "359    -0.034053        w10_rank_v_L84_avg"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coef = pd.DataFrame({'Feature':include_columns,'Coefficient': lm.coef_})\n",
    "model_coef.sort_values(by='Coefficient',inplace=True,ascending=False) \n",
    "model_coef.reset_index(drop=True,inplace=True)\n",
    "model_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model to predict for  2015-02-05\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0747\n",
      "evaluation error: 0.0635\n",
      "R-squared score (training): 0.103\n",
      "R-squared score (test): 0.238\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0497\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "139    0.960784    0.687726    0.010741  0.060291\n",
      "70     0.986928    0.671993    0.010741  0.071770\n",
      "67     0.882353    0.642061    0.010741  0.031835\n",
      "45     0.888889    0.639172    0.010741  0.036401\n",
      "66     0.947712    0.637022    0.010741  0.048248\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0258\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "62    0.418301    0.383973    0.010741 -0.006829\n",
      "26    0.424837    0.378687    0.010741 -0.006699\n",
      "57    0.039216    0.363467    0.010741 -0.085190\n",
      "39    0.287582    0.345077    0.010741 -0.016544\n",
      "23    0.326797    0.294466    0.010741 -0.013518\n",
      "********************************************************\n",
      "Building model to predict for  2015-03-09\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0718\n",
      "evaluation error: 0.0813\n",
      "R-squared score (training): 0.137\n",
      "R-squared score (test): 0.024\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0901\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "126    0.986928    0.663506     0.00266  0.178278\n",
      "39     0.928105    0.655941     0.00266  0.082243\n",
      "6      0.915033    0.633398     0.00266  0.076116\n",
      "0      0.908497    0.624279     0.00266  0.075889\n",
      "96     0.692810    0.621342     0.00266  0.037737\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0355\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "32     0.973856    0.364592     0.00266  0.120546\n",
      "37     0.241830    0.358079     0.00266  0.009546\n",
      "132    0.732026    0.354664     0.00266  0.041502\n",
      "62     0.019608    0.330309     0.00266 -0.024067\n",
      "23     0.555556    0.283912     0.00266  0.030000\n",
      "********************************************************\n",
      "Building model to predict for  2015-04-08\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0699\n",
      "evaluation error: 0.0864\n",
      "R-squared score (training): 0.161\n",
      "R-squared score (test): -0.037\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0007\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "46     0.960784    0.775337    0.004202  0.055012\n",
      "103    0.065359    0.756036    0.004202 -0.040441\n",
      "83     0.830065    0.714165    0.004202  0.015691\n",
      "70     0.183007    0.708259    0.004202 -0.022443\n",
      "37     0.424837    0.704806    0.004202 -0.004433\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0299\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "98     0.039216    0.242316    0.004202 -0.057728\n",
      "55     0.921569    0.214322    0.004202  0.031329\n",
      "145    0.137255    0.210138    0.004202 -0.028592\n",
      "148    0.143791    0.202236    0.004202 -0.028416\n",
      "146    0.026144    0.193774    0.004202 -0.066340\n",
      "********************************************************\n",
      "Building model to predict for  2015-05-07\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0695\n",
      "evaluation error: 0.0562\n",
      "R-squared score (training): 0.166\n",
      "R-squared score (test): 0.326\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0015\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "66    0.967320    0.865942   -0.001902  0.038891\n",
      "47    0.281046    0.777095   -0.001902 -0.023092\n",
      "72    0.052288    0.749825   -0.001902 -0.069268\n",
      "25    0.921569    0.724551   -0.001902  0.025511\n",
      "88    0.960784    0.704083   -0.001902  0.035501\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.05\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "55     0.111111    0.320885   -0.001902 -0.050082\n",
      "143    0.352941    0.264029   -0.001902 -0.016112\n",
      "80     0.235294    0.226267   -0.001902 -0.029327\n",
      "57     0.032680    0.168287   -0.001902 -0.086982\n",
      "148    0.058824    0.136310   -0.001902 -0.067742\n",
      "********************************************************\n",
      "Building model to predict for  2015-06-08\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0665\n",
      "evaluation error: 0.113\n",
      "R-squared score (training): 0.201\n",
      "R-squared score (test): -0.357\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.1205\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "54    0.235294    0.819818    -0.01377 -0.052173\n",
      "55    0.111111    0.758589    -0.01377 -0.076194\n",
      "15    0.006536    0.740659    -0.01377 -0.367158\n",
      "97    0.320261    0.740011    -0.01377 -0.042537\n",
      "75    0.130719    0.715173    -0.01377 -0.064442\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0145\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "141    0.849673    0.364607    -0.01377  0.005923\n",
      "67     0.967320    0.362332    -0.01377  0.022563\n",
      "104    1.000000    0.312347    -0.01377  0.042595\n",
      "37     0.137255    0.292938    -0.01377 -0.063771\n",
      "139    0.098039    0.275440    -0.01377 -0.079822\n",
      "********************************************************\n",
      "Building model to predict for  2015-07-08\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0644\n",
      "evaluation error: 0.0788\n",
      "R-squared score (training): 0.225\n",
      "R-squared score (test): 0.054\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.05\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "120    0.660131    0.765174    0.018605  0.022837\n",
      "84     0.751634    0.742326    0.018605  0.031509\n",
      "15     1.000000    0.724840    0.018605  0.187063\n",
      "83     0.843137    0.716998    0.018605  0.045770\n",
      "101    0.098039    0.716390    0.018605 -0.037147\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0156\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "0      0.359477    0.244469    0.018605  0.000860\n",
      "6      0.313725    0.219861    0.018605 -0.001301\n",
      "126    0.803922    0.207095    0.018605  0.041403\n",
      "60     0.732026    0.195444    0.018605  0.029782\n",
      "11     0.457516    0.153298    0.018605  0.007485\n",
      "********************************************************\n",
      "Building model to predict for  2015-08-06\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0633\n",
      "evaluation error: 0.0688\n",
      "R-squared score (training): 0.239\n",
      "R-squared score (test): 0.175\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0458\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "104    0.947712    0.863565   -0.076446  0.005111\n",
      "95     0.686275    0.819192   -0.076446 -0.060969\n",
      "139    0.725490    0.810470   -0.076446 -0.053717\n",
      "103    0.712418    0.801994   -0.076446 -0.054852\n",
      "114    0.647059    0.780815   -0.076446 -0.064639\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.1261\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "60     0.013072    0.121976   -0.076446 -0.162802\n",
      "18     0.228758    0.119204   -0.076446 -0.105676\n",
      "126    0.032680    0.113233   -0.076446 -0.154767\n",
      "32     0.673203    0.057259   -0.076446 -0.063880\n",
      "11     0.071895   -0.106518   -0.076446 -0.143239\n",
      "********************************************************\n",
      "Building model to predict for  2015-09-04\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0624\n",
      "evaluation error: 0.0997\n",
      "R-squared score (training): 0.250\n",
      "R-squared score (test): -0.197\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0823\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "133    0.967320    0.788232    0.032631  0.095076\n",
      "79     0.843137    0.781065    0.032631  0.062937\n",
      "71     0.960784    0.774019    0.032631  0.093914\n",
      "151    0.875817    0.746785    0.032631  0.070399\n",
      "57     0.941176    0.722094    0.032631  0.089282\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0333\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "11     0.934641    0.250509    0.032631  0.085328\n",
      "101    0.026144    0.239999    0.032631 -0.041594\n",
      "32     0.882353    0.231971    0.032631  0.070767\n",
      "62     0.385621    0.228855    0.032631  0.018900\n",
      "78     0.601307    0.217387    0.032631  0.033308\n",
      "********************************************************\n",
      "Building model to predict for  2015-10-06\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0642\n",
      "evaluation error: 0.108\n",
      "R-squared score (training): 0.228\n",
      "R-squared score (test): -0.296\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0248\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "79     0.111111    0.817256    0.063911 -0.008892\n",
      "131    0.405229    0.657769    0.063911  0.018380\n",
      "90     0.986928    0.654065    0.063911  0.089200\n",
      "20     0.477124    0.652402    0.063911  0.026787\n",
      "80     0.222222    0.650565    0.063911 -0.001387\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0454\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "11     0.503268    0.270985    0.063911  0.027485\n",
      "17     0.509804    0.252538    0.063911  0.027884\n",
      "126    0.771242    0.236205    0.063911  0.054756\n",
      "60     0.758170    0.228537    0.063911  0.054163\n",
      "15     0.849673    0.217891    0.063911  0.062717\n",
      "********************************************************\n",
      "Building model to predict for  2015-11-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0652\n",
      "evaluation error: 0.098\n",
      "R-squared score (training): 0.217\n",
      "R-squared score (test): -0.176\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0377\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "143    0.091503    0.845038   -0.003687 -0.057326\n",
      "50     0.084967    0.825343   -0.003687 -0.057692\n",
      "62     0.026144    0.727150   -0.003687 -0.094929\n",
      "139    0.947712    0.716341   -0.003687  0.028113\n",
      "89     0.444444    0.662159   -0.003687 -0.006480\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0197\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "58    0.366013    0.340657   -0.003687 -0.010321\n",
      "51    0.169935    0.335746   -0.003687 -0.036261\n",
      "47    0.241830    0.334160   -0.003687 -0.020913\n",
      "48    0.196078    0.263068   -0.003687 -0.029400\n",
      "15    0.653595    0.252169   -0.003687 -0.001742\n",
      "********************************************************\n",
      "Building model to predict for  2015-12-04\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.066\n",
      "evaluation error: 0.132\n",
      "R-squared score (training): 0.207\n",
      "R-squared score (test): -0.580\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0581\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "11    0.222222    0.870704   -0.046016 -0.074912\n",
      "15    0.607843    0.811764   -0.046016 -0.038236\n",
      "72    0.771242    0.806470   -0.046016 -0.011894\n",
      "57    0.045752    0.798633   -0.046016 -0.111326\n",
      "39    0.424837    0.769747   -0.046016 -0.053945\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.011\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "59     0.856209    0.227752   -0.046016 -0.000060\n",
      "24     0.699346    0.225767   -0.046016 -0.023972\n",
      "138    0.620915    0.224055   -0.046016 -0.036770\n",
      "102    0.816993    0.204238   -0.046016 -0.000901\n",
      "69     0.928105    0.195322   -0.046016  0.006667\n",
      "********************************************************\n",
      "Building model to predict for  2016-01-06\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0663\n",
      "evaluation error: 0.105\n",
      "R-squared score (training): 0.203\n",
      "R-squared score (test): -0.257\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0925\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "41     0.169935    0.711909   -0.054663 -0.077506\n",
      "67     0.326797    0.662956   -0.054663 -0.059386\n",
      "117    0.071895    0.658002   -0.054663 -0.112000\n",
      "84     0.039216    0.656001   -0.054663 -0.121793\n",
      "83     0.137255    0.642749   -0.054663 -0.091890\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0119\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "53    0.718954    0.324392   -0.054663 -0.004398\n",
      "44    0.973856    0.301898   -0.054663  0.059621\n",
      "27    0.633987    0.301750   -0.054663 -0.024027\n",
      "35    0.555556    0.297039   -0.054663 -0.039128\n",
      "23    0.980392    0.263349   -0.054663  0.067271\n",
      "********************************************************\n",
      "Building model to predict for  2016-02-05\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0656\n",
      "evaluation error: 0.111\n",
      "R-squared score (training): 0.212\n",
      "R-squared score (test): -0.329\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0456\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "145    0.130719    0.755174    0.055758  0.004795\n",
      "73     0.052288    0.751008    0.055758 -0.000586\n",
      "150    0.509804    0.716285    0.055758  0.049184\n",
      "104    0.934641    0.714987    0.055758  0.097434\n",
      "83     0.803922    0.711470    0.055758  0.077209\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0286\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "58    0.254902    0.282490    0.055758  0.022332\n",
      "46    0.333333    0.269426    0.055758  0.032056\n",
      "38    0.281046    0.247502    0.055758  0.027119\n",
      "48    0.300654    0.230054    0.055758  0.029451\n",
      "23    0.346405    0.228986    0.055758  0.032186\n",
      "********************************************************\n",
      "Building model to predict for  2016-03-08\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0682\n",
      "evaluation error: 0.0821\n",
      "R-squared score (training): 0.181\n",
      "R-squared score (test): 0.015\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0353\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "117    0.111111    0.779716    0.033627 -0.005695\n",
      "71     0.941176    0.766432    0.033627  0.059693\n",
      "87     0.915033    0.753628    0.033627  0.055297\n",
      "67     0.333333    0.745243    0.033627  0.014635\n",
      "83     0.895425    0.721486    0.033627  0.052428\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0154\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "29     0.764706    0.282944    0.033627  0.036592\n",
      "46     0.660131    0.258687    0.033627  0.032411\n",
      "102    0.686275    0.230369    0.033627  0.033325\n",
      "69     0.084967    0.227968    0.033627 -0.015612\n",
      "138    0.098039    0.177692    0.033627 -0.009576\n",
      "********************************************************\n",
      "Building model to predict for  2016-04-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvkrein/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0663\n",
      "evaluation error: 0.109\n",
      "R-squared score (training): 0.204\n",
      "R-squared score (test): -0.305\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.016\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "70     0.006536    0.944500    0.008977 -0.090559\n",
      "88     0.013072    0.912192    0.008977 -0.079832\n",
      "117    0.921569    0.725384    0.008977  0.061634\n",
      "24     0.111111    0.718892    0.008977 -0.011566\n",
      "66     0.830065    0.691768    0.008977  0.040135\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.001\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "17    0.202614    0.246423    0.008977 -0.001238\n",
      "53    0.150327    0.240671    0.008977 -0.007082\n",
      "6     0.176471    0.231694    0.008977 -0.005029\n",
      "50    0.045752    0.175869    0.008977 -0.038139\n",
      "32    0.901961    0.049648    0.008977  0.056305\n",
      "********************************************************\n",
      "Building model to predict for  2016-05-06\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0669\n",
      "evaluation error: 0.118\n",
      "R-squared score (training): 0.197\n",
      "R-squared score (test): -0.418\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0495\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "70     0.960784    0.885735    0.028819  0.091978\n",
      "88     0.986928    0.862835    0.028819  0.114706\n",
      "89     0.601307    0.776241    0.028819  0.036516\n",
      "59     0.117647    0.709957    0.028819  0.001526\n",
      "127    0.150327    0.704695    0.028819  0.002900\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.057\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "57    0.915033    0.303105    0.028819  0.071921\n",
      "75    0.973856    0.285766    0.028819  0.096401\n",
      "11    0.679739    0.231052    0.028819  0.039563\n",
      "53    0.758170    0.230431    0.028819  0.043194\n",
      "32    0.535948    0.125875    0.028819  0.033784\n",
      "********************************************************\n",
      "Building model to predict for  2016-06-07\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0674\n",
      "evaluation error: 0.0557\n",
      "R-squared score (training): 0.190\n",
      "R-squared score (test): 0.331\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0663\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "146    0.980392    0.827899   -0.005191  0.077340\n",
      "145    0.941176    0.731947   -0.005191  0.040056\n",
      "69     0.993464    0.728833   -0.005191  0.093411\n",
      "71     0.954248    0.710969   -0.005191  0.058194\n",
      "133    0.973856    0.707873   -0.005191  0.062289\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0657\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "78     0.143791    0.283312   -0.005191 -0.077806\n",
      "11     0.542484    0.278194   -0.005191 -0.008762\n",
      "68     0.098039    0.269721   -0.005191 -0.085344\n",
      "117    0.130719    0.269338   -0.005191 -0.080719\n",
      "75     0.156863    0.199372   -0.005191 -0.075920\n",
      "********************************************************\n",
      "Building model to predict for  2016-07-07\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0695\n",
      "evaluation error: 0.0707\n",
      "R-squared score (training): 0.166\n",
      "R-squared score (test): 0.142\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.1172\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "32    0.529412    0.775073    0.041489  0.048595\n",
      "97    0.960784    0.770095    0.041489  0.119874\n",
      "44    0.254902    0.757081    0.041489  0.016030\n",
      "54    0.986928    0.728612    0.041489  0.175680\n",
      "55    0.993464    0.709179    0.041489  0.225937\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0052\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "144    0.058824    0.377062    0.041489 -0.009565\n",
      "62     0.006536    0.362552    0.041489 -0.040761\n",
      "138    0.176471    0.345300    0.041489  0.000000\n",
      "99     0.666667    0.302068    0.041489  0.059052\n",
      "65     0.261438    0.298701    0.041489  0.017231\n",
      "********************************************************\n",
      "Building model to predict for  2016-08-05\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0683\n",
      "evaluation error: 0.0695\n",
      "R-squared score (training): 0.180\n",
      "R-squared score (test): 0.166\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0597\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "76     0.895425    0.790863    0.004421  0.051487\n",
      "40     0.888889    0.730848    0.004421  0.051188\n",
      "46     0.980392    0.728913    0.004421  0.084112\n",
      "24     0.967320    0.713633    0.004421  0.071390\n",
      "135    0.797386    0.710378    0.004421  0.040238\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0088\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "62    0.431373    0.348714    0.004421  0.010623\n",
      "18    0.653595    0.310486    0.004421  0.021873\n",
      "23    0.032680    0.277332    0.004421 -0.014706\n",
      "69    0.392157    0.271363    0.004421  0.009310\n",
      "50    0.562092    0.242506    0.004421  0.016861\n",
      "********************************************************\n",
      "Building model to predict for  2016-09-06\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0659\n",
      "evaluation error: 0.097\n",
      "R-squared score (training): 0.208\n",
      "R-squared score (test): -0.164\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0158\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "62     0.993464    0.759468   -0.010794  0.059566\n",
      "89     0.725490    0.741585   -0.010794 -0.001543\n",
      "81     0.738562    0.738111   -0.010794 -0.001150\n",
      "75     0.986928    0.686680   -0.010794  0.037321\n",
      "103    0.287582    0.667689   -0.010794 -0.015063\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0069\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "23    0.915033    0.266698   -0.010794  0.015308\n",
      "49    0.222222    0.262853   -0.010794 -0.020721\n",
      "16    0.424837    0.261227   -0.010794 -0.009945\n",
      "53    0.196078    0.240452   -0.010794 -0.022215\n",
      "50    0.836601    0.233755   -0.010794  0.003189\n",
      "********************************************************\n",
      "Building model to predict for  2016-10-05\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0639\n",
      "evaluation error: 0.0995\n",
      "R-squared score (training): 0.233\n",
      "R-squared score (test): -0.194\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.07\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "88     0.013072    0.807155   -0.031675 -0.147890\n",
      "89     0.052288    0.739245   -0.031675 -0.073389\n",
      "93     0.215686    0.725398   -0.031675 -0.047674\n",
      "25     0.156863    0.715522   -0.031675 -0.055869\n",
      "131    0.594771    0.699900   -0.031675 -0.025112\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0115\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "29     0.725490    0.280796   -0.031675 -0.014595\n",
      "139    0.516340    0.268072   -0.031675 -0.029354\n",
      "53     0.176471    0.250037   -0.031675 -0.054110\n",
      "97     0.973856    0.209054   -0.031675  0.023529\n",
      "54     0.967320    0.199008   -0.031675  0.016896\n",
      "********************************************************\n",
      "Building model to predict for  2016-11-03\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0654\n",
      "evaluation error: 0.0772\n",
      "R-squared score (training): 0.215\n",
      "R-squared score (test): 0.073\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0335\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "72     0.156863    0.800837    0.057947 -0.044100\n",
      "88     0.816993    0.788606    0.057947  0.073719\n",
      "70     0.875817    0.783593    0.057947  0.101511\n",
      "25     0.692810    0.781996    0.057947  0.042553\n",
      "126    0.431373    0.737669    0.057947 -0.006272\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0395\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "85     0.098039    0.354501    0.057947 -0.055249\n",
      "102    0.169935    0.347528    0.057947 -0.040367\n",
      "69     0.026144    0.344409    0.057947 -0.102789\n",
      "33     0.745098    0.334560    0.057947  0.051399\n",
      "47     0.117647    0.299166    0.057947 -0.050741\n",
      "********************************************************\n",
      "Building model to predict for  2016-12-05\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0661\n",
      "evaluation error: 0.094\n",
      "R-squared score (training): 0.206\n",
      "R-squared score (test): -0.129\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0376\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "70     0.392157    0.840124    0.030848  0.023041\n",
      "117    0.673203    0.797107    0.030848  0.035125\n",
      "76     0.908497    0.756975    0.030848  0.058481\n",
      "25     0.405229    0.747793    0.030848  0.024013\n",
      "24     0.836601    0.742621    0.030848  0.047545\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0227\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "62    0.313725    0.267815    0.030848  0.018229\n",
      "81    0.228758    0.244482    0.030848  0.014274\n",
      "14    0.039216    0.239812    0.030848 -0.011878\n",
      "92    0.366013    0.231859    0.030848  0.022477\n",
      "28    0.947712    0.182606    0.030848  0.070645\n",
      "********************************************************\n",
      "Building model to predict for  2017-01-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0676\n",
      "evaluation error: 0.117\n",
      "R-squared score (training): 0.188\n",
      "R-squared score (test): -0.399\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0012\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "33    0.183007    0.784279    0.011592 -0.001021\n",
      "76    0.653595    0.744395    0.011592  0.015639\n",
      "47    0.156863    0.737503    0.011592 -0.001833\n",
      "48    0.104575    0.734727    0.011592 -0.006385\n",
      "58    0.202614    0.734512    0.011592 -0.000284\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0447\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "138    0.960784    0.244132    0.011592  0.067259\n",
      "143    0.784314    0.241766    0.011592  0.028379\n",
      "30     0.790850    0.240194    0.011592  0.028412\n",
      "14     0.934641    0.238695    0.011592  0.056055\n",
      "31     0.849673    0.134429    0.011592  0.043210\n",
      "********************************************************\n",
      "Building model to predict for  2017-02-06\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0701\n",
      "evaluation error: 0.108\n",
      "R-squared score (training): 0.159\n",
      "R-squared score (test): -0.300\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.001\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "136    0.326797    0.747874    0.033467 -0.000780\n",
      "137    0.274510    0.739824    0.033467 -0.002911\n",
      "59     0.398693    0.731436    0.033467  0.002760\n",
      "141    0.405229    0.730103    0.033467  0.003220\n",
      "125    0.176471    0.722300    0.033467 -0.007041\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0169\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "55     0.764706    0.287480    0.033467  0.027904\n",
      "143    0.052288    0.284377    0.033467 -0.030343\n",
      "139    0.575163    0.283081    0.033467  0.013592\n",
      "101    0.091503    0.270176    0.033467 -0.018127\n",
      "31     0.019608    0.201413    0.033467 -0.077487\n",
      "********************************************************\n",
      "Building model to predict for  2017-03-08\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0726\n",
      "evaluation error: 0.105\n",
      "R-squared score (training): 0.128\n",
      "R-squared score (test): -0.254\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0069\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "112    0.326797    0.698479   -0.000638  0.005918\n",
      "118    0.045752    0.682654   -0.000638 -0.014789\n",
      "114    0.215686    0.657333   -0.000638  0.000518\n",
      "29     0.908497    0.655861   -0.000638  0.047098\n",
      "95     0.104575    0.638340   -0.000638 -0.004113\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0427\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "99    0.973856    0.388993   -0.000638  0.076494\n",
      "44    0.516340    0.375010   -0.000638  0.016032\n",
      "90    0.281046    0.366288   -0.000638  0.003375\n",
      "23    0.967320    0.358844   -0.000638  0.072304\n",
      "28    0.888889    0.269787   -0.000638  0.045535\n",
      "********************************************************\n",
      "Building model to predict for  2017-04-06\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0743\n",
      "evaluation error: 0.0669\n",
      "R-squared score (training): 0.108\n",
      "R-squared score (test): 0.197\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0788\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "33    0.980392    0.802441    0.018024  0.092040\n",
      "40    0.967320    0.743039    0.018024  0.088374\n",
      "22    0.836601    0.676895    0.018024  0.050742\n",
      "24    0.947712    0.673803    0.018024  0.084792\n",
      "29    0.921569    0.665748    0.018024  0.078146\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0437\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "28    0.862745    0.294702    0.018024  0.052602\n",
      "62    0.026144    0.289182    0.018024 -0.062166\n",
      "81    0.045752    0.284116    0.018024 -0.041703\n",
      "75    0.039216    0.229818    0.018024 -0.052983\n",
      "78    0.006536    0.180126    0.018024 -0.114306\n",
      "********************************************************\n",
      "Building model to predict for  2017-05-08\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.074\n",
      "evaluation error: 0.0878\n",
      "R-squared score (training): 0.112\n",
      "R-squared score (test): -0.054\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0165\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "40     0.307190    0.664381    0.016918  0.010826\n",
      "52     0.928105    0.649404    0.016918  0.047050\n",
      "72     0.215686    0.615223    0.016918  0.005821\n",
      "151    0.529412    0.609627    0.016918  0.017780\n",
      "66     0.143791    0.606742    0.016918  0.001179\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0009\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "28     0.281046    0.390619    0.016918  0.009941\n",
      "89     0.614379    0.382485    0.016918  0.023484\n",
      "79     0.901961    0.365293    0.016918  0.044360\n",
      "103    0.411765    0.354145    0.016918  0.014653\n",
      "78     0.006536    0.336468    0.016918 -0.087813\n",
      "********************************************************\n",
      "Building model to predict for  2017-06-07\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0728\n",
      "evaluation error: 0.101\n",
      "R-squared score (training): 0.126\n",
      "R-squared score (test): -0.209\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0006\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "72    0.777778    0.691137   -0.001747  0.008710\n",
      "45    0.281046    0.682695   -0.001747 -0.010426\n",
      "77    0.300654    0.676570   -0.001747 -0.009700\n",
      "76    0.333333    0.672899   -0.001747 -0.008712\n",
      "40    0.895425    0.655460   -0.001747  0.023072\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0043\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "112    0.928105    0.299949   -0.001747  0.033861\n",
      "95     0.862745    0.293087   -0.001747  0.015875\n",
      "96     0.758170    0.287617   -0.001747  0.007227\n",
      "94     0.803922    0.271987   -0.001747  0.011455\n",
      "78     0.032680    0.253856   -0.001747 -0.046993\n",
      "********************************************************\n",
      "Building model to predict for  2017-07-07\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0734\n",
      "evaluation error: 0.0736\n",
      "R-squared score (training): 0.118\n",
      "R-squared score (test): 0.117\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0648\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "138    0.699346    0.747226     0.02384  0.040733\n",
      "148    0.986928    0.678113     0.02384  0.123233\n",
      "152    0.830065    0.649754     0.02384  0.053209\n",
      "80     0.790850    0.646258     0.02384  0.049929\n",
      "79     0.856209    0.644416     0.02384  0.056917\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0408\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "81     0.575163    0.358573     0.02384  0.028893\n",
      "112    0.032680    0.338005     0.02384 -0.018804\n",
      "78     0.091503    0.337908     0.02384 -0.000307\n",
      "54     0.993464    0.333954     0.02384  0.139125\n",
      "32     0.843137    0.279196     0.02384  0.055074\n",
      "********************************************************\n",
      "Building model to predict for  2017-08-07\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0728\n",
      "evaluation error: 0.0853\n",
      "R-squared score (training): 0.126\n",
      "R-squared score (test): -0.024\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0035\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "89     0.771242    0.736958   -0.003647  0.010811\n",
      "148    0.895425    0.680016   -0.003647  0.024945\n",
      "43     0.150327    0.650937   -0.003647 -0.013245\n",
      "25     0.398693    0.637007   -0.003647 -0.001242\n",
      "52     0.326797    0.630894   -0.003647 -0.003724\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0439\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "14     0.934641    0.359028   -0.003647  0.044643\n",
      "103    0.267974    0.345227   -0.003647 -0.004628\n",
      "78     0.202614    0.343458   -0.003647 -0.008901\n",
      "97     0.960784    0.291645   -0.003647  0.055622\n",
      "55     1.000000    0.225569   -0.003647  0.132673\n",
      "********************************************************\n",
      "Building model to predict for  2017-09-06\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0731\n",
      "evaluation error: 0.118\n",
      "R-squared score (training): 0.122\n",
      "R-squared score (test): -0.421\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0066\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "52     0.032680    0.767041    0.036537 -0.038804\n",
      "0      0.633987    0.698954    0.036537  0.030797\n",
      "138    0.013072    0.678037    0.036537 -0.069947\n",
      "6      0.803922    0.670825    0.036537  0.043931\n",
      "126    0.908497    0.666600    0.036537  0.067208\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0803\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "122    0.901961    0.293226    0.036537  0.066300\n",
      "55     0.542484    0.280617    0.036537  0.026224\n",
      "94     0.954248    0.263127    0.036537  0.092923\n",
      "95     0.967320    0.261886    0.036537  0.101030\n",
      "78     1.000000    0.235718    0.036537  0.115174\n",
      "********************************************************\n",
      "Building model to predict for  2017-10-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0742\n",
      "evaluation error: 0.0847\n",
      "R-squared score (training): 0.109\n",
      "R-squared score (test): -0.016\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0117\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "52     0.019608    0.743993    0.014822 -0.071667\n",
      "138    0.725490    0.737055    0.014822  0.014659\n",
      "29     0.745098    0.721161    0.014822  0.015203\n",
      "126    0.359477    0.686885    0.014822  0.001369\n",
      "148    0.091503    0.672909    0.014822 -0.018113\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0027\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "117    0.836601    0.334034    0.014822  0.020555\n",
      "95     0.084967    0.328446    0.014822 -0.018722\n",
      "66     0.189542    0.317539    0.014822 -0.009052\n",
      "75     0.934641    0.306532    0.014822  0.037583\n",
      "78     0.052288    0.269663    0.014822 -0.043677\n",
      "********************************************************\n",
      "Building model to predict for  2017-11-03\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0736\n",
      "evaluation error: 0.0918\n",
      "R-squared score (training): 0.117\n",
      "R-squared score (test): -0.101\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0104\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "52     0.555556    0.794730    0.018487  0.004588\n",
      "25     0.941176    0.764625    0.018487  0.034239\n",
      "126    0.117647    0.726377    0.018487 -0.016406\n",
      "148    0.058824    0.676954    0.018487 -0.035922\n",
      "37     0.052288    0.660975    0.018487 -0.038256\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0167\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "137    0.366013    0.373080    0.018487 -0.002151\n",
      "136    0.437908    0.370655    0.018487  0.000553\n",
      "104    1.000000    0.369378    0.018487  0.089537\n",
      "41     0.405229    0.352041    0.018487 -0.000507\n",
      "62     0.274510    0.349825    0.018487 -0.003819\n",
      "********************************************************\n",
      "Building model to predict for  2017-12-05\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0737\n",
      "evaluation error: 0.0659\n",
      "R-squared score (training): 0.115\n",
      "R-squared score (test): 0.209\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0836\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "148    0.986928    0.746126    0.043916  0.133615\n",
      "99     0.797386    0.720123    0.043916  0.067873\n",
      "37     0.392157    0.716141    0.043916  0.036315\n",
      "46     0.954248    0.703143    0.043916  0.101562\n",
      "139    0.843137    0.691821    0.043916  0.078567\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0519\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "81     0.875817    0.297747    0.043916  0.085723\n",
      "94     0.267974    0.288847    0.043916  0.024607\n",
      "75     0.947712    0.286241    0.043916  0.098219\n",
      "95     0.248366    0.245942    0.043916  0.021928\n",
      "112    0.313725    0.244429    0.043916  0.028979\n",
      "********************************************************\n",
      "Building model to predict for  2018-01-05\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0725\n",
      "evaluation error: 0.0914\n",
      "R-squared score (training): 0.130\n",
      "R-squared score (test): -0.097\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0154\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "104    0.006536    0.682285   -0.014782 -0.093819\n",
      "39     0.477124    0.672080   -0.014782 -0.017348\n",
      "52     0.941176    0.663469   -0.014782  0.020792\n",
      "126    0.836601    0.663067   -0.014782  0.005806\n",
      "90     0.862745    0.657304   -0.014782  0.007660\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0388\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "118    0.117647    0.335074   -0.014782 -0.036975\n",
      "137    0.790850    0.332550   -0.014782 -0.001240\n",
      "31     0.405229    0.317295   -0.014782 -0.018970\n",
      "81     0.039216    0.297276   -0.014782 -0.069426\n",
      "78     0.052288    0.156548   -0.014782 -0.067536\n",
      "********************************************************\n",
      "Building model to predict for  2018-02-06\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.072\n",
      "evaluation error: 0.0811\n",
      "R-squared score (training): 0.136\n",
      "R-squared score (test): -0.012\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0003\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "104    0.058824    0.703509    0.017953 -0.037759\n",
      "126    0.196078    0.689266    0.017953 -0.010701\n",
      "148    0.130719    0.662607    0.017953 -0.015253\n",
      "103    0.941176    0.652357    0.017953  0.053501\n",
      "39     0.627451    0.649420    0.017953  0.008631\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0003\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "86     0.366013    0.270947    0.017953 -0.002467\n",
      "136    0.470588    0.260610    0.017953  0.000944\n",
      "74     0.424837    0.252935    0.017953 -0.001136\n",
      "137    0.431373    0.247178    0.017953 -0.000884\n",
      "141    0.503268    0.245280    0.017953  0.001814\n",
      "********************************************************\n",
      "Building model to predict for  2018-03-08\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0727\n",
      "evaluation error: 0.107\n",
      "R-squared score (training): 0.127\n",
      "R-squared score (test): -0.284\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0103\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "47     0.777778    0.745353   -0.044343  0.005199\n",
      "126    0.150327    0.738638   -0.044343 -0.045403\n",
      "48     0.758170    0.726304   -0.044343  0.003772\n",
      "58     0.607843    0.718866   -0.044343 -0.003862\n",
      "38     0.522876    0.698714   -0.044343 -0.011322\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0107\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "141    0.627451    0.328093   -0.044343 -0.001813\n",
      "145    0.895425    0.321665   -0.044343  0.013203\n",
      "74     0.764706    0.310710   -0.044343  0.004313\n",
      "133    0.993464    0.300818   -0.044343  0.028421\n",
      "73     0.875817    0.300396   -0.044343  0.009544\n",
      "********************************************************\n",
      "Building model to predict for  2018-04-09\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0744\n",
      "evaluation error: 0.0818\n",
      "R-squared score (training): 0.107\n",
      "R-squared score (test): 0.019\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0478\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "103    0.366013    0.701844    0.022764 -0.001938\n",
      "148    0.006536    0.679904    0.022764 -0.164500\n",
      "126    0.686275    0.660680    0.022764  0.017892\n",
      "29     0.026144    0.659732    0.022764 -0.063679\n",
      "57     0.137255    0.653325    0.022764 -0.026591\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0163\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "64     0.196078    0.378269    0.022764 -0.014201\n",
      "71     0.816993    0.348023    0.022764  0.028887\n",
      "133    0.823529    0.340451    0.022764  0.029447\n",
      "151    0.901961    0.333376    0.022764  0.038359\n",
      "72     0.385621    0.310393    0.022764 -0.001026\n",
      "********************************************************\n",
      "Building model to predict for  2018-05-08\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0741\n",
      "evaluation error: 0.0923\n",
      "R-squared score (training): 0.111\n",
      "R-squared score (test): -0.108\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0024\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "148    0.065359    0.740979    0.039378 -0.035907\n",
      "46     0.078431    0.724549    0.039378 -0.031086\n",
      "6      0.699346    0.698408    0.039378  0.028807\n",
      "16     0.267974    0.692398    0.039378  0.000000\n",
      "50     0.908497    0.686559    0.039378  0.050207\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0108\n",
      "    true_value  pred_value  mkt_return   returns\n",
      "74    0.392157    0.344876    0.039378  0.004487\n",
      "62    0.215686    0.340133    0.039378 -0.005666\n",
      "81    0.666667    0.245910    0.039378  0.026825\n",
      "75    0.627451    0.196023    0.039378  0.024161\n",
      "78    0.372549    0.156545    0.039378  0.004237\n",
      "********************************************************\n",
      "Building model to predict for  2018-06-07\n",
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0721\n",
      "evaluation error: 0.118\n",
      "R-squared score (training): 0.135\n",
      "R-squared score (test): -0.413\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: 0.0042\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "14     0.111111    0.834649    0.006674 -0.046723\n",
      "46     0.209150    0.822047    0.006674 -0.025759\n",
      "143    0.026144    0.775402    0.006674 -0.092768\n",
      "40     0.875817    0.771458    0.006674  0.022205\n",
      "52     1.000000    0.762662    0.006674  0.164132\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: 0.0194\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "25     0.771242    0.334105    0.006674  0.011274\n",
      "151    0.947712    0.325087    0.006674  0.041643\n",
      "71     0.960784    0.318987    0.006674  0.042953\n",
      "139    0.130719    0.302194    0.006674 -0.045480\n",
      "133    0.967320    0.295131    0.006674  0.046386\n",
      "********************************************************\n",
      "Building model to predict for  2018-07-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Eval Results:\n",
      "*****************************\n",
      "training error: 0.0726\n",
      "evaluation error: 0.0913\n",
      "R-squared score (training): 0.128\n",
      "R-squared score (test): -0.096\n",
      "*****************************\n",
      "Model Top 5 Picks\n",
      " Avg rtn top 5: -0.0197\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "14     0.581699    0.764939    0.027376  0.007524\n",
      "148    0.006536    0.730704    0.027376 -0.129697\n",
      "57     0.183007    0.702717    0.027376 -0.011736\n",
      "32     0.026144    0.688421    0.027376 -0.038342\n",
      "55     0.986928    0.671372    0.027376  0.073643\n",
      "Model Bottom 5 Picks\n",
      " Avg rtn bottom 5: -0.0131\n",
      "     true_value  pred_value  mkt_return   returns\n",
      "146    0.078431    0.358858    0.027376 -0.024801\n",
      "112    0.045752    0.356439    0.027376 -0.032514\n",
      "75     0.150327    0.326414    0.027376 -0.013146\n",
      "25     0.699346    0.316986    0.027376  0.018281\n",
      "84     0.143791    0.314509    0.027376 -0.013180\n",
      "********************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(dt1,dt_end,21):\n",
    "    print(\"Building model to predict for \",dates[i+273])\n",
    "    x_train = etf_data.loc[((etf_data['Date']>=dates[i]) & (etf_data['Date']<dates[i+252])),include_columns] #train with 12 mos\n",
    "    y_train = etf_data.loc[((etf_data['Date']>=dates[i]) & (etf_data['Date']<dates[i+252])),['rank_p_L-21']] #train with 12 mos\n",
    "\n",
    "    x_test = etf_data.loc[(etf_data['Date']==dates[i+273]),include_columns]#predict one day-must be 21 days removed from training\n",
    "    y_test = etf_data.loc[(etf_data['Date']==dates[i+273]),['rank_p_L-21']] #predict if etf >= market\n",
    "    returns = etf_data.loc[(etf_data['Date']==dates[i+273]),['delta_p_L-21']]\n",
    "    mkt_return = etf_data.loc[(etf_data['Date']==dates[i+273]),['ivv_delta_p_L-21']]\n",
    "    x_train_nmpy = x_train.as_matrix()\n",
    "    y_train_nmpy = np.ravel(y_train.as_matrix())\n",
    "\n",
    "    x_test_nmpy = x_test.as_matrix()\n",
    "    y_test_nmpy = np.ravel(y_test.as_matrix())\n",
    "    returns_nmpy = returns.as_matrix()\n",
    "    mkt_return_nmpy = mkt_return.as_matrix()\n",
    "\n",
    "#     lm = linear_model.Ridge (alpha = 0.8)\n",
    "    lm = linear_model.Lasso (alpha = 0.00019)\n",
    "#     lm = LinearRegression()\n",
    "\n",
    "    lm.fit(x_train_nmpy, y_train_nmpy)\n",
    "    \n",
    "    y_train_model = lm.predict(x_train_nmpy)\n",
    "    y_pred_model = lm.predict(x_test_nmpy)\n",
    "\n",
    "\n",
    "    y_check = np.column_stack((y_test_nmpy, y_pred_model,mkt_return_nmpy,returns_nmpy))\n",
    "    y_check_df = pd.DataFrame(y_check,columns=['true_value','pred_value','mkt_return','returns'])\n",
    "    y_check_df.sort_values('pred_value',inplace=True,ascending=False)\n",
    "    \n",
    "\n",
    "    train_error = mean_squared_error(y_train_nmpy,lm.predict(x_train_nmpy))\n",
    "    eval_error = mean_squared_error(y_test_nmpy,y_pred_model)\n",
    "\n",
    "    print('Model Eval Results:')\n",
    "    print('*****************************')\n",
    "\n",
    "    print(f'training error: {train_error:.3}')\n",
    "    print(f'evaluation error: {eval_error:.3}')\n",
    "    print('R-squared score (training): {:.3f}'\n",
    "     .format(r2_score(y_train_nmpy,y_train_model)))\n",
    "    print('R-squared score (test): {:.3f}'\n",
    "     .format(r2_score(y_test_nmpy, y_pred_model)))\n",
    "\n",
    "    print('*****************************')\n",
    "    print(\"Model Top 5 Picks\")\n",
    "    print(\" Avg rtn top 5:\", round(y_check_df['returns'][0:5].mean(),4))\n",
    "    print(y_check_df.head(5))\n",
    "    print(\"Model Bottom 5 Picks\")\n",
    "    print(\" Avg rtn bottom 5:\", round(y_check_df['returns'][-5:].mean(),4))\n",
    "    print(y_check_df.tail(5))\n",
    "    print('********************************************************')\n",
    "    if i == dt1:\n",
    "        predict_data = etf_data.loc[(etf_data['Date']==dates[i+273]),['Date','sym','rank_p_L-21']]\n",
    "        predict_data['predict'] = y_pred_model\n",
    "        predict_data.to_csv(etf_predict_file)\n",
    "    else:\n",
    "        predict_data = etf_data.loc[(etf_data['Date']==dates[i+273]),['Date','sym','rank_p_L-21']]\n",
    "        predict_data['predict'] = y_pred_model\n",
    "        predict_data.to_csv(etf_predict_file,header=False,mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4093587584945939"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.593016</td>\n",
       "      <td>w42_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.590215</td>\n",
       "      <td>w42_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.403456</td>\n",
       "      <td>w42_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.391652</td>\n",
       "      <td>w42_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.362188</td>\n",
       "      <td>w126_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.348371</td>\n",
       "      <td>w126_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.327921</td>\n",
       "      <td>w126_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.312515</td>\n",
       "      <td>w252_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.272693</td>\n",
       "      <td>w126_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.256563</td>\n",
       "      <td>w252_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.247219</td>\n",
       "      <td>w252_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.246534</td>\n",
       "      <td>w63_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.239817</td>\n",
       "      <td>w252_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.227262</td>\n",
       "      <td>w63_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.223118</td>\n",
       "      <td>w126_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.194516</td>\n",
       "      <td>w126_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.151981</td>\n",
       "      <td>w84_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.150993</td>\n",
       "      <td>w84_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.130270</td>\n",
       "      <td>w10_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.128146</td>\n",
       "      <td>w10_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.111784</td>\n",
       "      <td>w21_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.111314</td>\n",
       "      <td>w126_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.108846</td>\n",
       "      <td>w42_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.099371</td>\n",
       "      <td>w21_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.096179</td>\n",
       "      <td>w21_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.091386</td>\n",
       "      <td>w126_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.066379</td>\n",
       "      <td>w63_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.065501</td>\n",
       "      <td>w63_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.058659</td>\n",
       "      <td>w21_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.051717</td>\n",
       "      <td>w21_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.045726</td>\n",
       "      <td>w21_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.039613</td>\n",
       "      <td>w252_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.039136</td>\n",
       "      <td>w42_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.038437</td>\n",
       "      <td>w5_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.029094</td>\n",
       "      <td>w252_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.025375</td>\n",
       "      <td>w42_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.016853</td>\n",
       "      <td>w42_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.016669</td>\n",
       "      <td>w10_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.010491</td>\n",
       "      <td>w10_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.009622</td>\n",
       "      <td>w5_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.009478</td>\n",
       "      <td>w5_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.009477</td>\n",
       "      <td>w5_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.009225</td>\n",
       "      <td>w5_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.003939</td>\n",
       "      <td>rank_p_L84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.003168</td>\n",
       "      <td>w5_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.003019</td>\n",
       "      <td>w5_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.001558</td>\n",
       "      <td>w5_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000510</td>\n",
       "      <td>rank_v_L84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w189_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w126_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w63_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w189_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w84_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w42_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w21_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w10_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w5_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w189_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_p_L05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w126_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w10_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w63_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w42_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w21_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w63_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w42_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w21_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w42_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w21_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w10_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w189_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w126_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w84_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w63_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w126_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w21_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w189_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w189_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w189_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w42_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w21_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w10_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w5_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w10_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w63_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w42_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w189_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w42_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w21_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w10_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w126_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w63_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w42_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w21_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_w189_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w252_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w84_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w126_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w189_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w5_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w10_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w63_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w126_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w189_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w252_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w126_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w5_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w21_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w84_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w189_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w189_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w10_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w21_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w63_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w189_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w252_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w42_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w84_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w126_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w189_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w252_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w5_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w21_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w252_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w189_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w10_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>rank_p_L21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>rank_p_L42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>rank_p_L63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>rank_p_L126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>rank_p_L189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>rank_p_L252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>rank_v_L21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>rank_v_L42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>rank_v_L63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>rank_v_L126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>rank_v_L189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>rank_v_L252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w10_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w21_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w84_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w189_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w252_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w42_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w84_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w126_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w189_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w252_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w5_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w189_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w84_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w126_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_rank_v_L189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w126_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w189_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_v_L126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w84_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_v_L84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_rank_p_L63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_p_L84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w21_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_p_L126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_rank_p_L189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>ivv_rank_v_L63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w5_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_v_L42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_p_L252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_v_L21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w126_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w252_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_v_L10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_v_L05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w10_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_v_L252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w189_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w42_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w189_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_p_L10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w63_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_p_L21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w63_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w189_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w252_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w21_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w10_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>w5_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w10_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_rank_p_L42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>ivv_w5_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>w21_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-0.000082</td>\n",
       "      <td>w10_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-0.001377</td>\n",
       "      <td>w42_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>-0.001510</td>\n",
       "      <td>w42_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-0.002068</td>\n",
       "      <td>w5_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>-0.002675</td>\n",
       "      <td>rank_v_L10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>-0.003578</td>\n",
       "      <td>w5_rank_p_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>-0.003898</td>\n",
       "      <td>rank_p_L10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>-0.005132</td>\n",
       "      <td>rank_p_L05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>-0.007758</td>\n",
       "      <td>rank_v_L05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>-0.014664</td>\n",
       "      <td>w10_rank_v_L10_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-0.022614</td>\n",
       "      <td>w42_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>-0.024415</td>\n",
       "      <td>w42_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>-0.032062</td>\n",
       "      <td>w21_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>-0.032194</td>\n",
       "      <td>w21_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>-0.034779</td>\n",
       "      <td>w126_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>-0.036318</td>\n",
       "      <td>w126_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>-0.045291</td>\n",
       "      <td>w189_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>-0.046469</td>\n",
       "      <td>w189_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>-0.048184</td>\n",
       "      <td>w84_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>-0.050375</td>\n",
       "      <td>w10_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>-0.051836</td>\n",
       "      <td>w84_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>-0.053916</td>\n",
       "      <td>w84_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>-0.054237</td>\n",
       "      <td>w10_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>-0.056245</td>\n",
       "      <td>w10_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>-0.063728</td>\n",
       "      <td>w84_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>-0.065947</td>\n",
       "      <td>w21_rank_v_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>-0.068912</td>\n",
       "      <td>w21_rank_p_L126_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>-0.072561</td>\n",
       "      <td>w10_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-0.081201</td>\n",
       "      <td>w84_rank_v_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>-0.086000</td>\n",
       "      <td>w5_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>-0.088195</td>\n",
       "      <td>w5_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>-0.106902</td>\n",
       "      <td>w42_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>-0.109046</td>\n",
       "      <td>w189_rank_v_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>-0.119370</td>\n",
       "      <td>w189_rank_p_L189_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>-0.122946</td>\n",
       "      <td>w84_rank_p_L42_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>-0.129211</td>\n",
       "      <td>w252_rank_p_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-0.131691</td>\n",
       "      <td>w42_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>-0.132377</td>\n",
       "      <td>w21_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>-0.139698</td>\n",
       "      <td>w10_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>-0.147917</td>\n",
       "      <td>w10_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>-0.150590</td>\n",
       "      <td>w21_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>-0.157129</td>\n",
       "      <td>w252_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>-0.159759</td>\n",
       "      <td>w252_rank_v_L63_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>-0.161437</td>\n",
       "      <td>w42_rank_p_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>-0.170346</td>\n",
       "      <td>w252_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>-0.351828</td>\n",
       "      <td>w84_rank_v_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>-0.371314</td>\n",
       "      <td>w42_rank_v_L05_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>-0.417417</td>\n",
       "      <td>w84_rank_p_L21_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>-0.474683</td>\n",
       "      <td>w84_rank_v_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>-0.488751</td>\n",
       "      <td>w84_rank_p_L84_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-0.593071</td>\n",
       "      <td>w63_rank_v_L252_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>-0.598376</td>\n",
       "      <td>w63_rank_p_L252_avg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coefficient                   Feature\n",
       "0       0.593016       w42_rank_p_L252_avg\n",
       "1       0.590215       w42_rank_v_L252_avg\n",
       "2       0.403456        w42_rank_p_L84_avg\n",
       "3       0.391652        w42_rank_v_L84_avg\n",
       "4       0.362188       w126_rank_p_L05_avg\n",
       "5       0.348371       w126_rank_p_L21_avg\n",
       "6       0.327921       w126_rank_v_L21_avg\n",
       "7       0.312515       w252_rank_p_L21_avg\n",
       "8       0.272693       w126_rank_v_L05_avg\n",
       "9       0.256563       w252_rank_v_L21_avg\n",
       "10      0.247219      w252_rank_p_L189_avg\n",
       "11      0.246534        w63_rank_p_L42_avg\n",
       "12      0.239817      w252_rank_v_L189_avg\n",
       "13      0.227262        w63_rank_v_L42_avg\n",
       "14      0.223118       w126_rank_v_L63_avg\n",
       "15      0.194516       w126_rank_p_L63_avg\n",
       "16      0.151981       w84_rank_p_L189_avg\n",
       "17      0.150993       w84_rank_v_L189_avg\n",
       "18      0.130270       w10_rank_p_L126_avg\n",
       "19      0.128146       w10_rank_v_L126_avg\n",
       "20      0.111784       w21_rank_p_L189_avg\n",
       "21      0.111314       w126_rank_v_L42_avg\n",
       "22      0.108846        w42_rank_p_L21_avg\n",
       "23      0.099371       w21_rank_v_L189_avg\n",
       "24      0.096179        w21_rank_v_L10_avg\n",
       "25      0.091386       w126_rank_p_L42_avg\n",
       "26      0.066379       w63_rank_p_L189_avg\n",
       "27      0.065501       w63_rank_v_L189_avg\n",
       "28      0.058659        w21_rank_p_L21_avg\n",
       "29      0.051717        w21_rank_p_L10_avg\n",
       "30      0.045726        w21_rank_v_L21_avg\n",
       "31      0.039613      w252_rank_p_L252_avg\n",
       "32      0.039136        w42_rank_v_L21_avg\n",
       "33      0.038437         w5_rank_p_L05_avg\n",
       "34      0.029094      w252_rank_v_L252_avg\n",
       "35      0.025375       w42_rank_p_L189_avg\n",
       "36      0.016853       w42_rank_v_L189_avg\n",
       "37      0.016669        w10_rank_p_L63_avg\n",
       "38      0.010491        w10_rank_v_L63_avg\n",
       "39      0.009622         w5_rank_p_L63_avg\n",
       "40      0.009478         w5_rank_p_L42_avg\n",
       "41      0.009477        w5_rank_p_L126_avg\n",
       "42      0.009225         w5_rank_v_L42_avg\n",
       "43      0.003939                rank_p_L84\n",
       "44      0.003168        w5_rank_v_L126_avg\n",
       "45      0.003019         w5_rank_v_L63_avg\n",
       "46      0.001558         w5_rank_v_L05_avg\n",
       "47      0.000510                rank_v_L84\n",
       "48      0.000000   ivv_w42_rank_p_L126_avg\n",
       "49      0.000000    ivv_w5_rank_p_L252_avg\n",
       "50      0.000000  ivv_w252_rank_p_L189_avg\n",
       "51      0.000000   ivv_w252_rank_p_L84_avg\n",
       "52      0.000000  ivv_w189_rank_p_L189_avg\n",
       "53      0.000000    ivv_w5_rank_p_L126_avg\n",
       "54      0.000000   ivv_w10_rank_p_L126_avg\n",
       "55      0.000000  ivv_w126_rank_p_L189_avg\n",
       "56      0.000000   ivv_w21_rank_p_L126_avg\n",
       "57     -0.000000  ivv_w126_rank_p_L126_avg\n",
       "58     -0.000000   ivv_w63_rank_p_L126_avg\n",
       "59      0.000000   ivv_w189_rank_p_L84_avg\n",
       "60      0.000000   ivv_w84_rank_p_L189_avg\n",
       "61     -0.000000   ivv_w84_rank_p_L126_avg\n",
       "62      0.000000   ivv_w63_rank_p_L189_avg\n",
       "63     -0.000000   ivv_w42_rank_p_L189_avg\n",
       "64     -0.000000   ivv_w21_rank_p_L189_avg\n",
       "65     -0.000000   ivv_w10_rank_p_L189_avg\n",
       "66     -0.000000    ivv_w5_rank_p_L189_avg\n",
       "67      0.000000  ivv_w252_rank_p_L126_avg\n",
       "68      0.000000  ivv_w189_rank_p_L126_avg\n",
       "69      0.000000            ivv_rank_p_L05\n",
       "70     -0.000000   ivv_w189_rank_p_L63_avg\n",
       "71     -0.000000   ivv_w126_rank_p_L84_avg\n",
       "72     -0.000000    ivv_w10_rank_p_L42_avg\n",
       "73      0.000000   ivv_w252_rank_p_L21_avg\n",
       "74     -0.000000   ivv_w189_rank_p_L21_avg\n",
       "75      0.000000   ivv_w126_rank_p_L21_avg\n",
       "76      0.000000    ivv_w84_rank_p_L21_avg\n",
       "77     -0.000000    ivv_w63_rank_p_L21_avg\n",
       "78     -0.000000    ivv_w42_rank_p_L21_avg\n",
       "79     -0.000000    ivv_w21_rank_p_L21_avg\n",
       "80      0.000000    ivv_w10_rank_p_L21_avg\n",
       "81      0.000000     ivv_w5_rank_p_L21_avg\n",
       "82      0.000000   ivv_w252_rank_p_L10_avg\n",
       "83     -0.000000   ivv_w189_rank_p_L10_avg\n",
       "84      0.000000   ivv_w126_rank_p_L10_avg\n",
       "85      0.000000    ivv_w84_rank_p_L10_avg\n",
       "86     -0.000000    ivv_w63_rank_p_L10_avg\n",
       "87     -0.000000    ivv_w42_rank_p_L10_avg\n",
       "88      0.000000    ivv_w21_rank_p_L10_avg\n",
       "89      0.000000    ivv_w10_rank_p_L10_avg\n",
       "90      0.000000     ivv_w5_rank_p_L10_avg\n",
       "91      0.000000   ivv_w252_rank_p_L05_avg\n",
       "92      0.000000     ivv_w5_rank_p_L42_avg\n",
       "93     -0.000000    ivv_w21_rank_p_L42_avg\n",
       "94      0.000000    ivv_w84_rank_p_L84_avg\n",
       "95     -0.000000    ivv_w42_rank_p_L42_avg\n",
       "96      0.000000    ivv_w63_rank_p_L84_avg\n",
       "97      0.000000    ivv_w42_rank_p_L84_avg\n",
       "98      0.000000    ivv_w21_rank_p_L84_avg\n",
       "99      0.000000    ivv_w10_rank_p_L84_avg\n",
       "100     0.000000     ivv_w5_rank_p_L84_avg\n",
       "101     0.000000   ivv_w252_rank_p_L63_avg\n",
       "102     0.000000   ivv_w21_rank_p_L252_avg\n",
       "103     0.000000   ivv_w126_rank_p_L63_avg\n",
       "104     0.000000    ivv_w84_rank_p_L63_avg\n",
       "105     0.000000    ivv_w63_rank_p_L63_avg\n",
       "106     0.000000    ivv_w42_rank_p_L63_avg\n",
       "107    -0.000000    ivv_w21_rank_p_L63_avg\n",
       "108    -0.000000    ivv_w10_rank_p_L63_avg\n",
       "109     0.000000     ivv_w5_rank_p_L63_avg\n",
       "110     0.000000   ivv_w252_rank_p_L42_avg\n",
       "111    -0.000000   ivv_w189_rank_p_L42_avg\n",
       "112     0.000000   ivv_w126_rank_p_L42_avg\n",
       "113     0.000000    ivv_w84_rank_p_L42_avg\n",
       "114     0.000000    ivv_w63_rank_p_L42_avg\n",
       "115     0.000000   ivv_w10_rank_p_L252_avg\n",
       "116     0.000000    ivv_w84_rank_v_L05_avg\n",
       "117     0.000000   ivv_w42_rank_p_L252_avg\n",
       "118     0.000000    ivv_w63_rank_v_L84_avg\n",
       "119     0.000000   ivv_w42_rank_v_L126_avg\n",
       "120     0.000000   ivv_w21_rank_v_L126_avg\n",
       "121     0.000000   ivv_w10_rank_v_L126_avg\n",
       "122     0.000000    ivv_w5_rank_v_L126_avg\n",
       "123     0.000000   ivv_w252_rank_v_L84_avg\n",
       "124     0.000000   ivv_w189_rank_v_L84_avg\n",
       "125    -0.000000   ivv_w126_rank_v_L84_avg\n",
       "126     0.000000    ivv_w84_rank_v_L84_avg\n",
       "127     0.000000    ivv_w42_rank_v_L84_avg\n",
       "128    -0.000000   ivv_w84_rank_v_L126_avg\n",
       "129     0.000000    ivv_w21_rank_v_L84_avg\n",
       "130     0.000000    ivv_w10_rank_v_L84_avg\n",
       "131     0.000000     ivv_w5_rank_v_L84_avg\n",
       "132     0.000000   ivv_w252_rank_v_L63_avg\n",
       "133    -0.000000   ivv_w189_rank_v_L63_avg\n",
       "134     0.000000   ivv_w126_rank_v_L63_avg\n",
       "135     0.000000    ivv_w84_rank_v_L63_avg\n",
       "136     0.000000    ivv_w63_rank_v_L63_avg\n",
       "137    -0.000000   ivv_w63_rank_v_L126_avg\n",
       "138    -0.000000  ivv_w126_rank_v_L126_avg\n",
       "139    -0.000000    ivv_w21_rank_v_L63_avg\n",
       "140     0.000000  ivv_w252_rank_v_L189_avg\n",
       "141     0.000000  ivv_w189_rank_v_L252_avg\n",
       "142     0.000000  ivv_w126_rank_v_L252_avg\n",
       "143     0.000000   ivv_w84_rank_v_L252_avg\n",
       "144     0.000000   ivv_w63_rank_v_L252_avg\n",
       "145     0.000000   ivv_w42_rank_v_L252_avg\n",
       "146     0.000000   ivv_w21_rank_v_L252_avg\n",
       "147     0.000000   ivv_w10_rank_v_L252_avg\n",
       "148     0.000000    ivv_w5_rank_v_L252_avg\n",
       "149     0.000000  ivv_w189_rank_v_L189_avg\n",
       "150     0.000000  ivv_w189_rank_v_L126_avg\n",
       "151     0.000000  ivv_w126_rank_v_L189_avg\n",
       "152     0.000000   ivv_w84_rank_v_L189_avg\n",
       "153     0.000000   ivv_w63_rank_v_L189_avg\n",
       "154    -0.000000   ivv_w42_rank_v_L189_avg\n",
       "155    -0.000000   ivv_w21_rank_v_L189_avg\n",
       "156    -0.000000   ivv_w10_rank_v_L189_avg\n",
       "157    -0.000000    ivv_w5_rank_v_L189_avg\n",
       "158     0.000000  ivv_w252_rank_v_L126_avg\n",
       "159     0.000000    ivv_w42_rank_v_L63_avg\n",
       "160    -0.000000    ivv_w10_rank_v_L63_avg\n",
       "161     0.000000   ivv_w63_rank_p_L252_avg\n",
       "162     0.000000   ivv_w126_rank_p_L05_avg\n",
       "163    -0.000000    ivv_w63_rank_v_L10_avg\n",
       "164    -0.000000    ivv_w42_rank_v_L10_avg\n",
       "165     0.000000    ivv_w21_rank_v_L10_avg\n",
       "166     0.000000    ivv_w10_rank_v_L10_avg\n",
       "167     0.000000     ivv_w5_rank_v_L10_avg\n",
       "168     0.000000   ivv_w252_rank_v_L05_avg\n",
       "169    -0.000000   ivv_w189_rank_v_L05_avg\n",
       "170     0.000000   ivv_w126_rank_v_L05_avg\n",
       "171     0.000000    ivv_w63_rank_v_L05_avg\n",
       "172     0.000000   ivv_w126_rank_v_L10_avg\n",
       "173     0.000000    ivv_w42_rank_v_L05_avg\n",
       "174     0.000000    ivv_w21_rank_v_L05_avg\n",
       "175     0.000000    ivv_w10_rank_v_L05_avg\n",
       "176     0.000000     ivv_w5_rank_v_L05_avg\n",
       "177     0.000000  ivv_w252_rank_p_L252_avg\n",
       "178     0.000000  ivv_w189_rank_p_L252_avg\n",
       "179     0.000000  ivv_w126_rank_p_L252_avg\n",
       "180     0.000000   ivv_w84_rank_p_L252_avg\n",
       "181     0.000000    ivv_w84_rank_v_L10_avg\n",
       "182    -0.000000   ivv_w189_rank_v_L10_avg\n",
       "183     0.000000     ivv_w5_rank_v_L63_avg\n",
       "184     0.000000     ivv_w5_rank_v_L42_avg\n",
       "185     0.000000   ivv_w252_rank_v_L42_avg\n",
       "186    -0.000000   ivv_w189_rank_v_L42_avg\n",
       "187     0.000000   ivv_w126_rank_v_L42_avg\n",
       "188     0.000000    ivv_w84_rank_v_L42_avg\n",
       "189     0.000000    ivv_w63_rank_v_L42_avg\n",
       "190    -0.000000    ivv_w42_rank_v_L42_avg\n",
       "191    -0.000000    ivv_w21_rank_v_L42_avg\n",
       "192    -0.000000    ivv_w10_rank_v_L42_avg\n",
       "193     0.000000   ivv_w252_rank_v_L21_avg\n",
       "194     0.000000   ivv_w252_rank_v_L10_avg\n",
       "195    -0.000000   ivv_w189_rank_v_L21_avg\n",
       "196     0.000000   ivv_w126_rank_v_L21_avg\n",
       "197     0.000000    ivv_w84_rank_v_L21_avg\n",
       "198    -0.000000    ivv_w63_rank_v_L21_avg\n",
       "199    -0.000000    ivv_w42_rank_v_L21_avg\n",
       "200    -0.000000    ivv_w21_rank_v_L21_avg\n",
       "201     0.000000    ivv_w10_rank_v_L21_avg\n",
       "202     0.000000     ivv_w5_rank_v_L21_avg\n",
       "203    -0.000000   ivv_w189_rank_p_L05_avg\n",
       "204     0.000000  ivv_w252_rank_v_L252_avg\n",
       "205     0.000000    ivv_w84_rank_p_L05_avg\n",
       "206    -0.000000      w126_rank_p_L252_avg\n",
       "207    -0.000000        w63_rank_p_L63_avg\n",
       "208     0.000000       w189_rank_p_L63_avg\n",
       "209     0.000000         w5_rank_p_L84_avg\n",
       "210    -0.000000        w10_rank_p_L84_avg\n",
       "211     0.000000        w63_rank_p_L84_avg\n",
       "212    -0.000000       w126_rank_p_L84_avg\n",
       "213     0.000000       w189_rank_p_L84_avg\n",
       "214    -0.000000       w63_rank_p_L126_avg\n",
       "215    -0.000000      w252_rank_p_L126_avg\n",
       "216    -0.000000      w126_rank_p_L189_avg\n",
       "217    -0.000000        w5_rank_p_L252_avg\n",
       "218     0.000000       w21_rank_p_L252_avg\n",
       "219     0.000000       w84_rank_p_L252_avg\n",
       "220    -0.000000      w189_rank_p_L252_avg\n",
       "221    -0.000000       w189_rank_v_L21_avg\n",
       "222    -0.000000        w10_rank_v_L05_avg\n",
       "223    -0.000000        w21_rank_v_L05_avg\n",
       "224    -0.000000        w63_rank_v_L05_avg\n",
       "225     0.000000    ivv_w63_rank_p_L05_avg\n",
       "226     0.000000       w189_rank_v_L05_avg\n",
       "227     0.000000       w252_rank_v_L05_avg\n",
       "228     0.000000        w42_rank_v_L10_avg\n",
       "229    -0.000000        w63_rank_v_L10_avg\n",
       "230    -0.000000        w84_rank_v_L10_avg\n",
       "231     0.000000       w126_rank_v_L10_avg\n",
       "232    -0.000000       w189_rank_v_L10_avg\n",
       "233     0.000000       w252_rank_v_L10_avg\n",
       "234     0.000000         w5_rank_v_L21_avg\n",
       "235    -0.000000        w21_rank_p_L63_avg\n",
       "236     0.000000       w252_rank_p_L42_avg\n",
       "237    -0.000000       w189_rank_p_L42_avg\n",
       "238    -0.000000        w10_rank_p_L42_avg\n",
       "239     0.000000                rank_p_L21\n",
       "240     0.000000                rank_p_L42\n",
       "241     0.000000                rank_p_L63\n",
       "242     0.000000               rank_p_L126\n",
       "243    -0.000000               rank_p_L189\n",
       "244    -0.000000               rank_p_L252\n",
       "245     0.000000                rank_v_L21\n",
       "246     0.000000                rank_v_L42\n",
       "247     0.000000                rank_v_L63\n",
       "248     0.000000               rank_v_L126\n",
       "249    -0.000000               rank_v_L189\n",
       "250    -0.000000               rank_v_L252\n",
       "251    -0.000000        w10_rank_p_L05_avg\n",
       "252    -0.000000        w21_rank_p_L05_avg\n",
       "253    -0.000000        w63_rank_p_L05_avg\n",
       "254     0.000000        w84_rank_p_L05_avg\n",
       "255     0.000000       w189_rank_p_L05_avg\n",
       "256     0.000000       w252_rank_p_L05_avg\n",
       "257     0.000000        w42_rank_p_L10_avg\n",
       "258    -0.000000        w63_rank_p_L10_avg\n",
       "259    -0.000000        w84_rank_p_L10_avg\n",
       "260     0.000000       w126_rank_p_L10_avg\n",
       "261    -0.000000       w189_rank_p_L10_avg\n",
       "262     0.000000       w252_rank_p_L10_avg\n",
       "263     0.000000         w5_rank_p_L21_avg\n",
       "264    -0.000000        w63_rank_p_L21_avg\n",
       "265    -0.000000       w189_rank_p_L21_avg\n",
       "266    -0.000000        w63_rank_v_L21_avg\n",
       "267     0.000000        w84_rank_v_L05_avg\n",
       "268    -0.000000       w126_rank_v_L84_avg\n",
       "269    -0.000000           ivv_rank_v_L189\n",
       "270    -0.000000      w126_rank_v_L252_avg\n",
       "271     0.000000       w189_rank_v_L84_avg\n",
       "272     0.000000           ivv_rank_v_L126\n",
       "273     0.000000       w84_rank_v_L252_avg\n",
       "274     0.000000            ivv_rank_v_L84\n",
       "275    -0.000000            ivv_rank_p_L63\n",
       "276     0.000000            ivv_rank_p_L84\n",
       "277     0.000000       w21_rank_v_L252_avg\n",
       "278     0.000000           ivv_rank_p_L126\n",
       "279    -0.000000           ivv_rank_p_L189\n",
       "280    -0.000000            ivv_rank_v_L63\n",
       "281    -0.000000       w63_rank_v_L126_avg\n",
       "282    -0.000000        w5_rank_v_L252_avg\n",
       "283     0.000000            ivv_rank_v_L42\n",
       "284     0.000000           ivv_rank_p_L252\n",
       "285     0.000000            ivv_rank_v_L21\n",
       "286    -0.000000      w126_rank_v_L189_avg\n",
       "287    -0.000000      w252_rank_v_L126_avg\n",
       "288     0.000000            ivv_rank_v_L10\n",
       "289     0.000000            ivv_rank_v_L05\n",
       "290    -0.000000        w10_rank_v_L42_avg\n",
       "291     0.000000           ivv_rank_v_L252\n",
       "292    -0.000000      w189_rank_v_L252_avg\n",
       "293     0.000000    ivv_w42_rank_p_L05_avg\n",
       "294    -0.000000       w189_rank_v_L42_avg\n",
       "295     0.000000            ivv_rank_p_L10\n",
       "296     0.000000        w63_rank_v_L84_avg\n",
       "297     0.000000            ivv_rank_p_L21\n",
       "298    -0.000000        w63_rank_v_L63_avg\n",
       "299     0.000000       w189_rank_v_L63_avg\n",
       "300     0.000000       w252_rank_v_L42_avg\n",
       "301     0.000000    ivv_w21_rank_p_L05_avg\n",
       "302     0.000000    ivv_w10_rank_p_L05_avg\n",
       "303     0.000000         w5_rank_v_L84_avg\n",
       "304    -0.000000        w10_rank_v_L84_avg\n",
       "305     0.000000            ivv_rank_p_L42\n",
       "306     0.000000     ivv_w5_rank_p_L05_avg\n",
       "307    -0.000000        w21_rank_v_L63_avg\n",
       "308    -0.000082        w10_rank_p_L10_avg\n",
       "309    -0.001377        w42_rank_p_L42_avg\n",
       "310    -0.001510        w42_rank_v_L42_avg\n",
       "311    -0.002068         w5_rank_v_L10_avg\n",
       "312    -0.002675                rank_v_L10\n",
       "313    -0.003578         w5_rank_p_L10_avg\n",
       "314    -0.003898                rank_p_L10\n",
       "315    -0.005132                rank_p_L05\n",
       "316    -0.007758                rank_v_L05\n",
       "317    -0.014664        w10_rank_v_L10_avg\n",
       "318    -0.022614       w42_rank_v_L126_avg\n",
       "319    -0.024415       w42_rank_p_L126_avg\n",
       "320    -0.032062        w21_rank_p_L42_avg\n",
       "321    -0.032194        w21_rank_v_L42_avg\n",
       "322    -0.034779      w126_rank_v_L126_avg\n",
       "323    -0.036318      w126_rank_p_L126_avg\n",
       "324    -0.045291      w189_rank_v_L126_avg\n",
       "325    -0.046469      w189_rank_p_L126_avg\n",
       "326    -0.048184        w84_rank_v_L63_avg\n",
       "327    -0.050375       w10_rank_p_L189_avg\n",
       "328    -0.051836       w84_rank_v_L126_avg\n",
       "329    -0.053916       w84_rank_p_L126_avg\n",
       "330    -0.054237       w10_rank_v_L189_avg\n",
       "331    -0.056245        w10_rank_v_L21_avg\n",
       "332    -0.063728        w84_rank_p_L63_avg\n",
       "333    -0.065947       w21_rank_v_L126_avg\n",
       "334    -0.068912       w21_rank_p_L126_avg\n",
       "335    -0.072561        w10_rank_p_L21_avg\n",
       "336    -0.081201        w84_rank_v_L42_avg\n",
       "337    -0.086000        w5_rank_v_L189_avg\n",
       "338    -0.088195        w5_rank_p_L189_avg\n",
       "339    -0.106902        w42_rank_p_L63_avg\n",
       "340    -0.109046      w189_rank_v_L189_avg\n",
       "341    -0.119370      w189_rank_p_L189_avg\n",
       "342    -0.122946        w84_rank_p_L42_avg\n",
       "343    -0.129211       w252_rank_p_L63_avg\n",
       "344    -0.131691        w42_rank_v_L63_avg\n",
       "345    -0.132377        w21_rank_v_L84_avg\n",
       "346    -0.139698       w10_rank_v_L252_avg\n",
       "347    -0.147917       w10_rank_p_L252_avg\n",
       "348    -0.150590        w21_rank_p_L84_avg\n",
       "349    -0.157129       w252_rank_p_L84_avg\n",
       "350    -0.159759       w252_rank_v_L63_avg\n",
       "351    -0.161437        w42_rank_p_L05_avg\n",
       "352    -0.170346       w252_rank_v_L84_avg\n",
       "353    -0.351828        w84_rank_v_L21_avg\n",
       "354    -0.371314        w42_rank_v_L05_avg\n",
       "355    -0.417417        w84_rank_p_L21_avg\n",
       "356    -0.474683        w84_rank_v_L84_avg\n",
       "357    -0.488751        w84_rank_p_L84_avg\n",
       "358    -0.593071       w63_rank_v_L252_avg\n",
       "359    -0.598376       w63_rank_p_L252_avg"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coef = pd.DataFrame({'Feature':include_columns,'Coefficient': lm.coef_})\n",
    "model_coef.sort_values(by='Coefficient',inplace=True,ascending=False) \n",
    "model_coef.reset_index(drop=True,inplace=True)\n",
    "model_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"498.88890210493145\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(y_test_nmpy, y_pred_model,marker= 'o', s=50, alpha=0.8)\n",
    "#plt.plot(y_test_nmpy, y_pred_model, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('True value (y)')\n",
    "plt.ylabel('Predicted value (y)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
